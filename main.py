# -*- coding: utf-8 -*-
"""mvp_forecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yexc3XV8u600ggjHm2BBdmDmeQ-0TClV
"""

import numpy as np
import pandas as pd
from pathlib import Path

import re, ast
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD, NMF

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

"""### 1 часть решения - добавить тинкеры в новости"""

candles = pd.read_csv("candles.csv")


display(candles.head())


tickers = candles["ticker"].unique().tolist()


ticker_patterns = {
    "AFLT": [r"(?:^|[^а-яa-zё])аэрофлот[а-яё]*", r"(?:^|[^а-яa-zё])aeroflot[a-z]*"],
    "ALRS": [r"(?:^|[^а-яa-zё])алрос[а-яё]*", r"(?:^|[^а-яa-zё])alrosa[a-z]*"],
    "CHMF": [r"(?:^|[^а-яa-zё])северстал[а-яё]*", r"(?:^|[^а-яa-zё])severstal[a-z]*", r"(?:^|[^а-яa-zё])chmf"],
    "GAZP": [r"(?:^|[^а-яa-zё])газпром[а-яё]*", r"(?:^|[^а-яa-zё])gazprom[a-z]*"],
    "GMKN": [
        r"(?:^|[^а-яa-zё])норникел[а-яё]*",
        r"(?:^|[^а-яa-zё])норильск[а-яё]*\sникел[а-яё]*",
        r"(?:^|[^а-яa-zё])nornickel[a-z]*",
        r"(?:^|[^а-яa-zё])gmkn"
    ],
    "LKOH": [r"(?:^|[^а-яa-zё])лукойл[а-яё]*", r"(?:^|[^а-яa-zё])lukoil[a-z]*"],
    "MAGN": [
        r"(?:^|[^а-яa-zё])ммк",
        r"(?:^|[^а-яa-zё])магнитогорск[а-яё]*\sметаллургическ[а-яё]*",
        r"(?:^|[^а-яa-zё])magn[a-z]*"
    ],
    "MGNT": [r"(?:^|[^а-яa-zё])магнит[а-яё]*", r"(?:^|[^а-яa-zё])magnit[a-z]*"],
    "MOEX": [r"(?:^|[^а-яa-zё])московск[а-яё]*\sбирж[а-яё]*", r"(?:^|[^а-яa-zё])moex[a-z]*"],
    "MTSS": [r"(?:^|[^а-яa-zё])мтс", r"(?:^|[^а-яa-zё])mts", r"(?:^|[^а-яa-zё])mobile\s+telesystems"],
    "NVTK": [r"(?:^|[^а-яa-zё])новат[еэ]к[а-яё]*", r"(?:^|[^а-яa-zё])novatek[a-z]*"],
    "PHOR": [r"(?:^|[^а-яa-zё])фосагр[а-яё]*", r"(?:^|[^а-яa-zё])phosagro[a-z]*"],
    "PLZL": [
        r"(?:^|[^а-яa-zё])полюс[а-яё]*\sзолот[а-яё]*",
        r"(?:^|[^а-яa-zё])polyus[a-z]*",
        r"(?:^|[^а-яa-zё])plzl"
    ],
    "ROSN": [r"(?:^|[^а-яa-zё])роснефт[а-яё]*", r"(?:^|[^а-яa-zё])rosneft[a-z]*"],
    "RUAL": [r"(?:^|[^а-яa-zё])русал[а-яё]*", r"(?:^|[^а-яa-zё])rusal[a-z]*"],
    "SBER": [
        r"(?:^|[^а-яa-zё])сбербанк[а-яё]*",
        r"(?:^|[^а-яa-zё])сбер[а-яё]*",
        r"(?:^|[^а-яa-zё])sber[a-z]*",
        r"(?:^|[^а-яa-zё])sberbank[a-z]*"
    ],
    "SIBN": [
        r"(?:^|[^а-яa-zё])газпром\s+нефт[а-яё]*",
        r"(?:^|[^а-яa-zё])gazprom\s+neft[a-z]*"
    ],
    "T": [
        r"(?:^|[^а-яa-zё])т-?банк[а-яё]*",
        r"(?:^|[^а-яa-zё])тинькофф[а-яё]*",
        r"(?:^|[^а-яa-zё])tinkoff[a-z]*",
        r"(?:^|[^а-яa-zё])t-?bank[a-z]*"
    ],
    "VTBR": [r"(?:^|[^а-яa-zё])втб", r"(?:^|[^а-яa-zё])vtb[a-z]*"],
}

# ===== Функция для поиска тикеров =====
def extract_tickers(text, mapping=ticker_patterns):
    text = str(text).lower()
    found = []
    for ticker, patterns in mapping.items():
        if any(re.search(p, text) for p in patterns):
            found.append(ticker)
    return list(set(found)) if found else None

# ===== Пути к файлам =====
train_news_path = "news.csv"
test_news_path = "news_2.csv"

# ===== Загружаем новости =====
train_news = pd.read_csv(train_news_path)
test_news = pd.read_csv(test_news_path)

# ===== Добавляем колонку tickers =====
def add_tickers_column(df):
    # Используем title + publication
    if "title" in df.columns and "publication" in df.columns:
        text_data = df["title"].fillna("") + " " + df["publication"].fillna("")
    elif "title" in df.columns:
        text_data = df["title"].fillna("")
    else:
        text_data = df.astype(str).agg(" ".join, axis=1)

    df["tickers"] = text_data.apply(extract_tickers)
    return df

train_news = add_tickers_column(train_news)
test_news = add_tickers_column(test_news)

# ===== Сохраняем =====
out_dir = Path("data/processed/")
out_dir.mkdir(parents=True, exist_ok=True)

train_out = out_dir / "train_news_with_tickers.csv"
test_out = out_dir / "test_news_with_tickers.csv"

train_news.to_csv(train_out, index=False)
test_news.to_csv(test_out, index=False)

print("✅ Файлы успешно сохранены:")

"""### 2 часть решения - разметка новостей"""

from __future__ import annotations

import os
import math
from dataclasses import dataclass, replace
from typing import List, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from tqdm.auto import tqdm
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification,
)

torch.backends.cuda.matmul.allow_tf32 = True
try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

__all__ = [
    "FinBertConfig",
    "RUN_FLAGS",
    "process_news_with_config",
    "run_with_flags",
    "detect_device",
]

@dataclass
class FinBertConfig:
    input_csv: str
    output_csv: str
    no_translate: bool = False            # True => без перевода, многоязычный сентимент
    batch_size: int = 64
    max_seq_len: int = 256
    max_new_tokens: int = 96
    truncate_publication: Optional[int] = 300  # 0/None => не обрезать publication
    use_ctranslate2: bool = False         # True => нужен ctranslate2 + конвертированная модель
    ctranslate2_dir: Optional[str] = None
    title_only: bool = False              # True => использовать только title

# правь пути/настройки под свой проект
RUN_FLAGS = FinBertConfig(
    input_csv="train_news_with_tickers.csv",
    output_csv="fast_train_news_with_sent.csv",
    no_translate=False,
    batch_size=64,
    max_seq_len=256,
    max_new_tokens=96,
    truncate_publication=300,
    use_ctranslate2=False,
    ctranslate2_dir=None,
    title_only=False,
)

RUN_FLAGS_TEST = FinBertConfig(
    input_csv="test_news_with_tickers.csv",
    output_csv="fast_test_news_with_sent.csv",
    no_translate=False,
    batch_size=64,
    max_seq_len=256,
    max_new_tokens=96,
    truncate_publication=300,
    use_ctranslate2=False,
    ctranslate2_dir=None,
    title_only=False,
)

# ------------------ Утилиты ------------------
def detect_device():
    dev = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[Device] torch={torch.__version__} | device={dev} | cuda={getattr(torch.version, 'cuda', None)}")
    if dev == "cuda":
        try:
            print(f"[GPU] {torch.cuda.get_device_name(0)} | mem={(torch.cuda.get_device_properties(0).total_memory/1e9):.1f} GB")
        except Exception as e:
            print(f"[GPU warn] {e}")
    return dev

def build_text(title, body, truncate_publication: Optional[int] = None):
    t = (str(title) if pd.notna(title) else "").strip()
    b = (str(body) if pd.notna(body) else "").strip()
    if truncate_publication and isinstance(truncate_publication, int) and truncate_publication > 0:
        b = b[:truncate_publication]
    if t and b:
        return f"{t}. {b}"
    return t or b

def safe_read_csv(path, encodings=("utf-8", "utf-8-sig", "cp1251")) -> pd.DataFrame:
    last_err = None
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception as e:
            last_err = e
    raise RuntimeError(f"Не удалось прочитать CSV '{path}'. Последняя ошибка: {last_err}")

# ------------------ Перевод RU→EN ------------------
def load_translation_model(device: str, model_name="Helsinki-NLP/opus-mt-ru-en"):
    tok = AutoTokenizer.from_pretrained(model_name)
    kwargs = {}
    if device == "cuda":
        kwargs["torch_dtype"] = torch.float16
    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name, **kwargs).to(device)
    if device == "cuda":
        mdl.half()
    else:
        from torch.quantization import quantize_dynamic
        mdl = quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8)
    mdl.eval()
    print("[OK] Translation model loaded:", model_name)
    print("Translator on:", next(mdl.parameters()).device)
    return tok, mdl

def translate_texts(texts: List[str], tok, mdl, device: str,
                    batch_size=32, max_len=512, max_new_tokens=128) -> List[str]:
    res = []
    total_batches = math.ceil(len(texts) / batch_size)
    for i in tqdm(range(0, len(texts), batch_size), total=total_batches, desc="Translate RU→EN (HF greedy)"):
        batch = texts[i:i+batch_size]
        inputs = tok(batch, return_tensors="pt", truncation=True, padding=True,
                     max_length=max_len, pad_to_multiple_of=8).to(device)
        with torch.inference_mode():
            gen = mdl.generate(
                **inputs,
                num_beams=1,
                do_sample=False,
                use_cache=True,
                max_new_tokens=max_new_tokens,
            )
        out = tok.batch_decode(gen, skip_special_tokens=True)
        res.extend(out)
    return res

# --- CTranslate2 (опционально) ---
def translate_ct2(texts: List[str], model_dir: str, device: str,
                  batch_size=128, max_len=256, max_new_tokens=96) -> List[str]:
    try:
        import ctranslate2
        import sentencepiece as spm
    except Exception as e:
        raise RuntimeError("Для use_ctranslate2 нужны пакеты: ctranslate2 и sentencepiece") from e

    sp_src = spm.SentencePieceProcessor()
    sp_tgt = spm.SentencePieceProcessor()
    sp_src.load(os.path.join(model_dir, "source.spm"))
    sp_tgt.load(os.path.join(model_dir, "target.spm"))

    compute_type = "float16" if device == "cuda" else "int8"
    translator = ctranslate2.Translator(model_dir, device=device, compute_type=compute_type)

    res = []
    total_batches = math.ceil(len(texts) / batch_size)
    for i in tqdm(range(0, len(texts), batch_size), total=total_batches, desc=f"Translate RU→EN (ct2 {compute_type})"):
        batch = texts[i:i+batch_size]
        src_tokens = [sp_src.encode(t, out_type=str)[:max_len] for t in batch]
        out = translator.translate_batch(src_tokens, beam_size=1, max_decoding_length=max_new_tokens)
        res.extend(sp_tgt.decode_pieces(o.hypotheses[0]) for o in out)
    return res

# ------------------ Сентимент ------------------
def load_finbert(device: str, model_name="ProsusAI/finbert"):
    tok = AutoTokenizer.from_pretrained(model_name)
    kwargs = {}
    if device == "cuda":
        kwargs["torch_dtype"] = torch.float16
    mdl = AutoModelForSequenceClassification.from_pretrained(model_name, **kwargs).to(device)
    if device == "cuda":
        mdl.half()
    else:
        from torch.quantization import quantize_dynamic
        mdl = quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8)
    mdl.eval()
    try:
        from optimum.bettertransformer import BetterTransformer
        mdl = BetterTransformer.transform(mdl, keep_original_model=False)
        print("[BT] BetterTransformer enabled")
    except Exception:
        pass
    # torch.compile (если доступно)
    try:
        if device == "cuda":
            mdl = torch.compile(mdl, mode="max-autotune")
            print("[Compile] torch.compile enabled")
    except Exception:
        pass

    id2label = {i: mdl.config.id2label[i] for i in range(mdl.config.num_labels)}
    label_names = [id2label[i].lower() for i in range(mdl.config.num_labels)]
    print("[OK] FinBERT loaded:", model_name, "| labels:", label_names)
    print("FinBERT on:", next(mdl.parameters()).device)
    return tok, mdl, label_names

def load_multilingual_sentiment(device: str, model_name="cardiffnlp/twitter-xlm-roberta-base-sentiment"):
    tok = AutoTokenizer.from_pretrained(model_name)
    kwargs = {}
    if device == "cuda":
        kwargs["torch_dtype"] = torch.float16
    mdl = AutoModelForSequenceClassification.from_pretrained(model_name, **kwargs).to(device)
    if device == "cuda":
        mdl.half()
    else:
        from torch.quantization import quantize_dynamic
        mdl = quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8)
    mdl.eval()
    try:
        from optimum.bettertransformer import BetterTransformer
        mdl = BetterTransformer.transform(mdl, keep_original_model=False)
        print("[BT] BetterTransformer enabled (multi)")
    except Exception:
        pass
    try:
        if device == "cuda":
            mdl = torch.compile(mdl, mode="max-autotune")
            print("[Compile] torch.compile enabled (multi)")
    except Exception:
        pass
    id2label = {i: mdl.config.id2label[i] for i in range(mdl.config.num_labels)}
    label_names = [id2label[i].lower() for i in range(mdl.config.num_labels)]
    print("[OK] Multilingual sentiment loaded:", model_name, "| labels:", label_names)
    print("Sentiment model on:", next(mdl.parameters()).device)
    return tok, mdl, label_names

def run_text_classifier(texts: List[str], tok, mdl, device: str,
                        batch_size=64, max_len=512, desc="Classifier") -> np.ndarray:
    total_batches = math.ceil(len(texts) / batch_size)
    out_chunks = []
    for start in tqdm(range(0, len(texts), batch_size), total=total_batches, desc=desc):
        batch = texts[start:start+batch_size]
        enc = tok(batch, return_tensors="pt", truncation=True, padding=True,
                  max_length=max_len, pad_to_multiple_of=8).to(device)
        with torch.inference_mode():
            logits = mdl(**enc).logits
            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()
        out_chunks.append(probs)
    return np.vstack(out_chunks) if out_chunks else np.empty((0, 0))

def process_news_with_config(cfg: FinBertConfig) -> pd.DataFrame:
    """
    Обрабатывает новости по конфигу cfg. Возвращает итоговый DataFrame и сохраняет CSV (+ .partial.csv).
    """
    device = detect_device()

    # 1) читаем CSV
    df = safe_read_csv(cfg.input_csv)
    required_cols = {"publish_date", "title", "publication", "tickers"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"В CSV отсутствуют столбцы: {missing}")

    # 2) собираем текст
    if cfg.title_only:
        df["__text_ru"] = df["title"].fillna("").astype(str)
    else:
        df["__text_ru"] = [
            build_text(t, p, truncate_publication=cfg.truncate_publication)
            for t, p in zip(df["title"], df["publication"])
        ]

    # Дедупликация
    text_codes, unique_texts = pd.factorize(df["__text_ru"], sort=False)
    unique_texts = list(unique_texts)

    # 3) Перевод (если нужен)
    if not cfg.no_translate:
        if cfg.use_ctranslate2:
            if not cfg.ctranslate2_dir or not os.path.isdir(cfg.ctranslate2_dir):
                raise ValueError("use_ctranslate2=True, но ctranslate2_dir не задан или не существует")
            unique_texts_en = translate_ct2(
                unique_texts, cfg.ctranslate2_dir, device,
                batch_size=max(64, cfg.batch_size),
                max_len=cfg.max_seq_len,
                max_new_tokens=cfg.max_new_tokens,
            )
        else:
            trans_tok, trans_mdl = load_translation_model(device)
            unique_texts_en = translate_texts(
                unique_texts, trans_tok, trans_mdl, device,
                batch_size=cfg.batch_size,
                max_len=cfg.max_seq_len,
                max_new_tokens=cfg.max_new_tokens,
            )
        texts_en = [unique_texts_en[c] for c in text_codes]
    else:
        texts_en = None

    # 4) Сентимент
    if not cfg.no_translate:
        fb_tok, fb_mdl, label_names = load_finbert(device)
        probs_unique = run_text_classifier(
            unique_texts_en, fb_tok, fb_mdl, device,
            batch_size=cfg.batch_size, max_len=cfg.max_seq_len, desc="FinBERT (EN)"
        )
        probs_out = probs_unique[text_codes]
    else:
        ml_tok, ml_mdl, label_names = load_multilingual_sentiment(device)
        probs_unique = run_text_classifier(
            unique_texts, ml_tok, ml_mdl, device,
            batch_size=cfg.batch_size, max_len=cfg.max_seq_len, desc="Multilingual sentiment (RU)"
        )
        probs_out = probs_unique[text_codes]

    # 5) Сохранение
    out_df = df.copy()
    for j, name in enumerate(label_names):
        out_df[f"p_{name.lower()}"] = probs_out[:, j]

    ln = [n.lower() for n in label_names]
    idx_pos = ln.index("positive") if "positive" in ln else None
    idx_neg = ln.index("negative") if "negative" in ln else None

    out_df["sent_label"] = [label_names[int(i)] for i in np.argmax(probs_out, axis=1)]
    if idx_pos is not None and idx_neg is not None:
        out_df["sent_score"] = probs_out[:, idx_pos] - probs_out[:, idx_neg]
    else:
        out_df["sent_score"] = np.nan

    out_df.drop(columns=["__text_ru"], inplace=True, errors="ignore")
    out_df.to_csv(cfg.output_csv + ".partial.csv", index=False)
    out_df.to_csv(cfg.output_csv, index=False)
    print(f"[OK] Saved: {cfg.output_csv}")
    print(f"[Note] Partial file: {cfg.output_csv}.partial.csv (оставлен)")
    return out_df

def run_with_flags(**overrides) -> pd.DataFrame:
    """
    Запуск с программными переопределениями дефолтов:
        run_with_flags(output_csv="...", no_translate=True)
    (не CLI, используется только при импорте в Python-коде)
    """
    cfg = replace(RUN_FLAGS, **overrides)
    return process_news_with_config(cfg)

print("[RUN] finbert.py запускается с параметрами RUN_FLAGS:")
print(RUN_FLAGS)
process_news_with_config(RUN_FLAGS)
process_news_with_config(RUN_FLAGS_TEST)

"""### 3 часть решения - разметим candles."""

def _pick_device(device: str = "auto"):
    import importlib
    import numpy as _np
    import pandas as _pd
    from scipy import sparse as _sp

    dev = "cpu"
    mods = {}

    if device in ("auto", "gpu"):
        try:
            _cp = importlib.import_module("cupy")
            _cudf = importlib.import_module("cudf")
            _cuml = importlib.import_module("cuml")
            _cupyx_sp = importlib.import_module("cupyx.scipy.sparse")

            tfidf_cls = importlib.import_module("cuml.feature_extraction.text").TfidfVectorizer
            svd_cls = importlib.import_module("cuml.decomposition").TruncatedSVD

            from sklearn.decomposition import NMF as _skNMF

            def _to_cpu_sparse(x):
                return x.get()
            def _to_cpu(x):
                return _cp.asnumpy(x)

            def _to_cudf_series(ps):
                return _cudf.Series(ps.astype(str).fillna(""))

            dev = "gpu"
            mods.update(dict(
                xp=_cp, pd=_pd, sp=_cupyx_sp,
                tfidf_cls=tfidf_cls, svd_cls=svd_cls, nmf_cls=_skNMF,
                to_cpu_sparse=_to_cpu_sparse, to_cpu=_to_cpu, to_cudf_series=_to_cudf_series,
                cudf=_cudf, cuml=_cuml
            ))
        except Exception:
            pass

    if dev == "cpu":
        from sklearn.feature_extraction.text import TfidfVectorizer as _skTFIDF
        from sklearn.decomposition import TruncatedSVD as _skSVD, NMF as _skNMF
        def _to_cpu_sparse(x): return x
        def _to_cpu(x): return x
        def _to_cudf_series(ps): return ps

        mods.update(dict(
            xp=_np, pd=_pd, sp=_sp,
            tfidf_cls=_skTFIDF, svd_cls=_skSVD, nmf_cls=_skNMF,
            to_cpu_sparse=_to_cpu_sparse, to_cpu=_to_cpu, to_cudf_series=_to_cudf_series
        ))
    return dev, mods

def hash_embed_publication(pub: str, dim: int = 8, seed: int = 42):
    rnd = np.random.RandomState(abs(hash((pub, seed))) % (2**32))
    v = rnd.normal(size=dim).astype(np.float32)
    v /= (np.linalg.norm(v) + 1e-12)
    return v

def softmax(x, xp=None):
    if xp is None:
        import numpy as np
        xp = np
    x = xp.asarray(x, dtype=xp.float32)
    if x.size:
        x = x - x.max()
    ex = xp.exp(x)
    return ex / (ex.sum() + 1e-12)

def transformer_like_pool(embs, times, now_ts, tau_days: float = 10.0, time_bias: float = 1.0, xp=None):
    import pandas as pd
    if xp is None:
        import numpy as _np
        xp = _np

    embs = xp.asarray(embs, dtype=xp.float32)
    if embs.ndim == 1:
        embs = embs.reshape(1, -1)
    if embs.shape[0] == 0:
        return xp.zeros((embs.shape[1],), dtype=xp.float32)

    D = embs.shape[1]
    q = embs.mean(axis=0).astype(xp.float32)
    scores = (embs @ q) / (xp.sqrt(D) + 1e-9)

    # даты считаем на CPU, затем переносим в xp
    times_ns = pd.to_datetime(times).values.astype('datetime64[ns]')
    now_ns   = pd.Timestamp(now_ts).to_datetime64().astype('datetime64[ns]')
    dt_days_cpu = ((now_ns - times_ns).astype('timedelta64[s]').astype('float64') / 86400.0)
    dt_days_cpu = dt_days_cpu.clip(min=0.0)

    dt_days = xp.asarray(dt_days_cpu, dtype=xp.float32)
    weights = softmax(scores + time_bias * (-dt_days / max(1e-6, tau_days)), xp=xp)
    ctx = (weights[:, None] * embs).sum(axis=0).astype(xp.float32)
    return ctx

def _norm_ticker(x: str) -> str:
    return re.sub(r"\s+", "", str(x).upper())

def _parse_tickers_cell(x):
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return []
    if isinstance(x, (list, tuple, set)):
        seq = list(x)
    else:
        s = str(x).strip()
        if not s:
            return []
        try:
            v = ast.literal_eval(s)
            if isinstance(v, (list, tuple, set)):
                seq = list(v)
            else:
                seq = [s]
        except Exception:
            seq = re.split(r"[,\|\s;]+", s.strip("[](){}'\" "))
    return [_norm_ticker(t) for t in seq if str(t).strip()]

class NewsVectorizer:
    """TF-IDF → SVD(text_dim) + NMF(n_topics). GPU if available (RAPIDS); NMF stays on CPU if needed."""
    def __init__(self, text_dim=128, n_topics=16, max_features=30000, random_state=42, device: str = "auto"):
        self.text_dim = text_dim
        self.n_topics = n_topics
        self.max_features = max_features
        self.random_state = random_state
        self.device, self.mods = _pick_device(device)
        self.vec = None
        self.svd = None
        self.nmf = None

    def fit(self, news_texts: pd.Series):
        pd = self.mods["pd"]
        tfidf_cls = self.mods["tfidf_cls"]
        nmf_cls = self.mods["nmf_cls"]
        to_cpu_sparse = self.mods["to_cpu_sparse"]
        to_cudf_series = self.mods["to_cudf_series"]

        from sklearn.decomposition import TruncatedSVD as _skSVD

        if self.device == "gpu":
            texts_gpu = to_cudf_series(news_texts.astype(str).fillna(''))
            self.vec = tfidf_cls(max_features=self.max_features, ngram_range=(1,2), min_df=2)
            X_gpu = self.vec.fit_transform(texts_gpu)
            X_cpu = to_cpu_sparse(X_gpu)

            self.svd = _skSVD(n_components=self.text_dim, random_state=self.random_state)
            self.svd.fit(X_cpu)

            if self.n_topics and self.n_topics > 0:
                self.nmf = nmf_cls(n_components=self.n_topics, init='nndsvda',
                                random_state=self.random_state, max_iter=300)
                self.nmf.fit(X_cpu)
        else:
            from sklearn.feature_extraction.text import TfidfVectorizer as _skTFIDF
            from sklearn.decomposition import TruncatedSVD as _skSVD
            self.vec = _skTFIDF(max_features=self.max_features, ngram_range=(1,2), min_df=2)
            X = self.vec.fit_transform(news_texts.astype(str).fillna(''))
            self.svd = _skSVD(n_components=self.text_dim, random_state=self.random_state)
            self.svd.fit(X)
            if self.n_topics and self.n_topics > 0:
                self.nmf = nmf_cls(n_components=self.n_topics, init='nndsvda',
                                random_state=self.random_state, max_iter=300)
                self.nmf.fit(X)
        return self


    def transform(self, texts: pd.Series):
        import numpy as np
        to_cpu_sparse = self.mods["to_cpu_sparse"]
        to_cudf_series = self.mods["to_cudf_series"]

        if self.device == "gpu":
            texts_gpu = to_cudf_series(texts.astype(str).fillna(''))
            X_gpu = self.vec.transform(texts_gpu)
            X_cpu = to_cpu_sparse(X_gpu)

            xs = self.svd.transform(X_cpu).astype(np.float32)
            if self.nmf is not None:
                xt = self.nmf.transform(X_cpu).astype(np.float32)
                xt /= (xt.sum(axis=1, keepdims=True) + 1e-9)
            else:
                xt = np.zeros((xs.shape[0], self.n_topics), dtype=np.float32)
            return xs, xt
        else:
            X = self.vec.transform(texts.astype(str).fillna(''))
            xs = self.svd.transform(X).astype(np.float32)
            if self.nmf is not None:
                xt = self.nmf.transform(X).astype(np.float32)
                xt /= (xt.sum(axis=1, keepdims=True) + 1e-9)
            else:
                xt = np.zeros((xs.shape[0], self.n_topics), dtype=np.float32)
            return xs, xt

def make_extra_news_features(day_df: pd.DataFrame, last_60d_df: pd.DataFrame | None, n_topics: int):
    feats = {}
    cnt = len(day_df)
    feats['news_count_day'] = int(cnt)

    if cnt == 0:
        feats['dup_ratio'] = 0.0
        feats['title_len_mean'] = 0.0
        feats['title_len_std']  = 0.0
        feats['publication_len_mean'] = 0.0
        feats['publication_len_std']  = 0.0
        feats['top_topic_idx'] = -1
        feats['top_topic_strength'] = 0.0
        for i in range(n_topics):
            feats[f'topic_mean_{i}'] = 0.0
        feats['news_spike_rough'] = 0.0
        return feats

    txt = (day_df['title'].astype(str).str.lower().str.strip() + ' ' +
           day_df['publication'].astype(str).str.lower().str.strip()).str.replace(r'\s+', ' ', regex=True)
    uniq = int(txt.nunique())
    feats['dup_ratio'] = float(1.0 - uniq / max(1, cnt))

    tlen = day_df['title'].astype(str).str.len()
    plen = day_df['publication'].astype(str).str.len()
    feats['title_len_mean'] = float(tlen.mean())
    feats['title_len_std']  = float(tlen.std() or 0.0)
    feats['publication_len_mean'] = float(plen.mean())
    feats['publication_len_std']  = float(plen.std() or 0.0)

    topic_mat = np.stack(day_df['topic_vec'].values, axis=0)  # (cnt, K)
    topic_mean = topic_mat.mean(axis=0)
    top_idx = int(np.argmax(topic_mean))
    feats['top_topic_idx'] = top_idx
    feats['top_topic_strength'] = float(topic_mean[top_idx])
    for i, v in enumerate(topic_mean):
        feats[f'topic_mean_{i}'] = float(v)

    if last_60d_df is not None and len(last_60d_df):
        days = last_60d_df['publish_date'].dt.normalize().nunique()
        avg_per_day = float(len(last_60d_df)) / max(1, int(days))
        feats['news_spike_rough'] = float(cnt / (avg_per_day + 1e-9))
    else:
        feats['news_spike_rough'] = float(cnt)
    return feats

def add_news_context_simple(
    candles_csv: str,
    news_csv: str,
    out_csv: str = None,
    text_dim: int = 128,
    n_topics: int = 16,
    max_news_per_sample: int = 128,
    tau_days: float = 10.0,
    add_aditional_emb: bool = True,
    add_context_emb: bool = True,
    device: str = "auto"
):
    """
    Если доступен GPU и установлен RAPIDS (cuDF/cuML), TF-IDF и SVD пойдут на GPU.
    NMF — по возможности на CPU (прозрачно), остальное — без изменений.
    """
    import numpy as np
    import pandas as pd
    from pathlib import Path

    dev, mods = _pick_device(device)
    xp = mods["xp"]

    # 1) свечи
    candles = pd.read_csv(candles_csv)
    candles['begin'] = pd.to_datetime(candles['begin'])
    candles['date'] = candles['begin'].dt.normalize()
    candles['ticker_norm'] = candles['ticker'].apply(_norm_ticker)

    # 2) новости
    news = pd.read_csv(news_csv)
    news['publish_date'] = pd.to_datetime(news['publish_date'])
    news['tickers_list'] = news['tickers'].apply(_parse_tickers_cell)

    out_parts = [candles]

    # =========================
    # A) КОНТЕКСТНЫЕ ЭМБЕДДИНГИ (из title И publication)
    # =========================
    if add_context_emb:
        vec_title = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['title'].astype(str).fillna('')
        )
        vec_pub   = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['publication'].astype(str).fillna('')
        )

        xs_t, xt_t = vec_title.transform(news['title'].astype(str).fillna(''))
        xs_p, xt_p = vec_pub.transform(news['publication'].astype(str).fillna(''))

        news_ctx = news[['publish_date','tickers_list']].copy()
        emb_np = np.concatenate([xs_t, xt_t, xs_p, xt_p], axis=1).astype(np.float32)
        news_ctx['emb'] = list(emb_np)

        D = 2*text_dim + 2*n_topics
        ctx_cols = [f'ctx_{i}' for i in range(D)]
        ctx_rows, diag_counts = [], []

        for row in candles.itertuples(index=False):
            tkr = row.ticker_norm
            cutoff = row.date - pd.Timedelta(days=1)
            mask = (news_ctx['publish_date'] <= cutoff) & (news_ctx['tickers_list'].apply(lambda lst: tkr in lst))
            past = news_ctx[mask]
            diag_counts.append(int(len(past)))
            if len(past) > max_news_per_sample:
                past = past.tail(max_news_per_sample)

            if len(past):
                # переносим эмбеддинги в xp для быстрой агрегации (если GPU)
                E = np.stack(past['emb'].values, axis=0).astype(np.float32)
                if dev == "gpu":
                    import cupy as cp
                    E_xp = cp.asarray(E)
                    T = pd.to_datetime(past['publish_date']).values.astype('datetime64[ns]')
                    ctx_xp = transformer_like_pool(E_xp, T, cutoff, tau_days=tau_days, time_bias=1.0, xp=cp)
                    ctx = cp.asnumpy(ctx_xp)
                else:
                    T = pd.to_datetime(past['publish_date']).values.astype('datetime64[ns]')
                    ctx = transformer_like_pool(E, T, cutoff, tau_days=tau_days, time_bias=1.0)
            else:
                ctx = np.zeros((D,), dtype=np.float32)
            ctx_rows.append(ctx)

        ctx_df = pd.DataFrame(np.vstack(ctx_rows), columns=ctx_cols, index=candles.index)
        out_parts += [ctx_df, pd.DataFrame({'ctx_news_count': diag_counts}, index=candles.index)]

    # =========================
    # B) ДОПОЛНИТЕЛЬНЫЕ ПРИЗНАКИ (темы считаем по ОБОИМ текстам)
    # =========================
    if add_aditional_emb:
        vec_feat_t = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['title'].astype(str).fillna('')
        )
        _, xt_t = vec_feat_t.transform(news['title'].astype(str).fillna(''))

        vec_feat_p = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['publication'].astype(str).fillna('')
        )
        _, xt_p = vec_feat_p.transform(news['publication'].astype(str).fillna(''))

        topic_vec = (xt_t + xt_p) / 2.0

        news_feat = news[['publish_date','title','publication','tickers_list']].copy()
        news_feat['topic_vec'] = list(topic_vec.astype(np.float32))

        extra_cols = (
            ['news_count_day','dup_ratio',
             'title_len_mean','title_len_std',
             'publication_len_mean','publication_len_std',
             'top_topic_idx','top_topic_strength','news_spike_rough'] +
            [f'topic_mean_{i}' for i in range(n_topics)]
        )
        extra_rows = []

        for row in candles.itertuples(index=False):
            tkr = row.ticker_norm
            cutoff = row.date - pd.Timedelta(days=1)

            mask_has = news_feat['tickers_list'].apply(lambda lst: tkr in lst)
            day_news = news_feat[(news_feat['publish_date'].dt.normalize() == cutoff) & mask_has].copy()
            last60   = news_feat[(news_feat['publish_date'] > (cutoff - pd.Timedelta(days=60))) &
                                 (news_feat['publish_date'] <= cutoff) & mask_has].copy()

            extra = make_extra_news_features(day_news, last60, n_topics)
            extra_rows.append([extra.get(c, 0.0) for c in extra_cols])

        extra_df = pd.DataFrame(extra_rows, columns=extra_cols, index=candles.index)
        out_parts.append(extra_df)

    enriched = pd.concat(out_parts, axis=1).drop(columns=['ticker_norm'], errors='ignore')

    if out_csv:
        Path(out_csv).parent.mkdir(parents=True, exist_ok=True)
        enriched.to_csv(out_csv, index=False)

    return enriched

enriched = add_news_context_simple(
    candles_csv="candles.csv",
    news_csv="fast_train_news_with_sent.csv",
    out_csv="candles_features.csv",
    text_dim=64,
    n_topics=12,
    max_news_per_sample=128,
    tau_days=7.0,
    add_context_emb = False,
    device="auto",
)

enriched = add_news_context_simple(
    candles_csv="candles_2.csv",
    news_csv="fast_test_news_with_sent.csv",
    out_csv="candles_2_features.csv",
    text_dim=64,
    n_topics=12,
    max_news_per_sample=128,
    tau_days=7.0,
    add_context_emb = False,
    device="auto",
)

"""### 4 часть решения - обучение модели и генерация submissions.csv"""

# -----------------------------
# 0) Вспомогательные
# -----------------------------
def _safe_parse_tickers(val):
    """Пытаемся превратить поле 'tickers' в список тикеров."""
    if pd.isna(val):
        return []
    s = str(val).strip()
    # 1) попытка как Python-список
    try:
        obj = ast.literal_eval(s)
        if isinstance(obj, (list, tuple)):
            return [str(x).strip().upper() for x in obj if str(x).strip()]
    except Exception:
        pass
    # 2) как строка: убираем скобки/кавычки и режем по не-буквенно-цифровым
    s = re.sub(r"[\[\]'\"{}]", " ", s)
    tokens = re.split(r"[^A-Za-z0-9\.]+", s)
    return [t.upper() for t in tokens if t.strip()]

# -----------------------------
# 1) Разметка таргетов R_1..R_20
# -----------------------------
def make_targets(df: pd.DataFrame, max_horizon: int = 20) -> pd.DataFrame:
    """
    Добавляет колонки R_1..R_max_horizon:
      R_i(t) = close_{t+i} / close_t - 1  (накопленная доходность за i дней)
    Делает всё по каждому тикеру отдельно.
    """
    out = df.copy()
    out = out.sort_values(["ticker", "begin"]).reset_index(drop=True)

    g = out.groupby("ticker", group_keys=False)
    for i in range(1, max_horizon + 1):
        out[f"R_{i}"] = g["close"].shift(-i) / out["close"] - 1.0

    return out

# -----------------------------------
# 2) Фичи из новостей (ежедневно и rolling)
# -----------------------------------
def build_news_features(
    news_csv: str,
    roll_days=(3, 7),
    use_cols=("p_positive", "p_negative", "p_neutral", "sent_score"),
):
    """
    Готовит фичи из новостей:
    - распарсенный тикер (explode)
    - агрегации по (ticker, date): среднее/макс/мин/sum sent_score, средние p_*, n_news
    - rolling по прошлым N дням, СДВИНУТЫЙ на 1 день (anti-leak)
    Возвращает датафрейм с колонками:
      ['ticker','date', base_feats..., <base>_lag1, <base>_roll{R}, ...]
    """
    print("📥 Loading news...")
    news = pd.read_csv(news_csv)
    news = news.drop(columns=["Unnamed: 0"], errors="ignore")

    news["publish_date"] = pd.to_datetime(news["publish_date"], errors="coerce")
    news = news.dropna(subset=["publish_date"])
    news["date"] = news["publish_date"].dt.normalize()

    # распарсить тикеры и explode
    tqdm.pandas(desc="🧷 Parse tickers")
    news["ticker_list"] = news["tickers"].progress_apply(_safe_parse_tickers)
    news = news.explode("ticker_list").rename(columns={"ticker_list": "ticker"})
    news["ticker"] = news["ticker"].astype(str)
    news = news[news["ticker"].str.len() > 0]

    # оставляем нужные колонки
    keep = ["ticker", "date", *use_cols]
    news = news[keep].copy()

    # базовые агрегации по дню
    agg_map = {
        "sent_score": ["mean", "max", "min", "sum"],
        "p_positive": ["mean"],
        "p_negative": ["mean"],
        "p_neutral": ["mean"],
    }
    print("📊 Daily aggregations...")
    daily = (
        news.groupby(["ticker", "date"])
        .agg(agg_map)
        .rename(columns={"mean": "mean", "max": "max", "min": "min", "sum": "sum"})
    )
    daily.columns = [f"{c0}_{c1}" for (c0, c1) in daily.columns.to_flat_index()]
    daily = daily.reset_index()

    # количество новостей
    cnt = news.groupby(["ticker", "date"]).size().reset_index(name="n_news")
    daily = daily.merge(cnt, on=["ticker", "date"], how="left")

    # гарантируем наличие всех фичей
    for c in ["sent_score_mean", "sent_score_max", "sent_score_min", "sent_score_sum",
              "p_positive_mean", "p_negative_mean", "p_neutral_mean", "n_news"]:
        if c not in daily.columns:
            daily[c] = 0.0

    # rolling по тикерам, сдвиг на 1 день (только прошлое)
    if roll_days:
        print("🔁 Rolling features...")
        pieces = []
        g = daily.sort_values(["ticker", "date"]).groupby("ticker", group_keys=False)
        for ticker, df_t in tqdm(g, total=g.ngroups, desc="Rolling per ticker"):
            df_t = df_t.sort_values("date").copy()
            base_cols = [
                "sent_score_mean", "sent_score_max", "sent_score_min", "sent_score_sum",
                "p_positive_mean", "p_negative_mean", "p_neutral_mean", "n_news",
            ]
            # лаг на 1 день
            for col in base_cols:
                df_t[f"{col}_lag1"] = df_t[col].shift(1)
            # средние по окнам, тоже сдвинутые
            for R in roll_days:
                for col in base_cols:
                    df_t[f"{col}_roll{R}"] = (
                        df_t[col].rolling(window=R, min_periods=1).mean().shift(1)
                    )
            pieces.append(df_t)
        daily = pd.concat(pieces, ignore_index=True)

    return daily  # ['ticker','date', base_today..., *_lag1, *_rollR]

# -----------------------------------
# 2.1) Календарные фичи
# -----------------------------------
def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Добавляет календарные числовые признаки на основе столбца 'date' (дата без времени).
    """
    out = df.copy()
    out["ds"] = pd.to_datetime(out["date"])
    out["dow"] = out["ds"].dt.weekday               # 0..6
    out["month"] = out["ds"].dt.month               # 1..12
    out["is_month_start"] = out["ds"].dt.is_month_start.astype(int)
    out["is_month_end"]   = out["ds"].dt.is_month_end.astype(int)
    # циклические кодировки дня недели
    out["dow_sin"] = np.sin(2*np.pi*out["dow"]/7.0)
    out["dow_cos"] = np.cos(2*np.pi*out["dow"]/7.0)
    return out

# -----------------------------------
# 3) Подготовка матрицы X / Y / сплит
# -----------------------------------
def build_xy(df_feat: pd.DataFrame, max_horizon: int = 20):
    """
    Собираем X (фичи) и Y (мультивыходной таргет R_1..R_max_horizon).
    Фичи: все числовые колонки, кроме R_*, begin (времени) и, на всякий случай, идентификаторов.
    """
    targets = [f"R_{i}" for i in range(1, max_horizon + 1)]
    # оставим только строки, где все таргеты присутствуют (чтобы модель видела полный вектор)
    df = df_feat.dropna(subset=targets).copy()

    # Числовые признаки
    X = df.select_dtypes(include=[np.number]).copy()
    # исключим сами таргеты из X
    X = X.drop(columns=[c for c in X.columns if c.startswith("R_")], errors="ignore")
    # begin/индексы/служебные — тоже не нужны в X
    X = X.drop(columns=["Unnamed: 0"], errors="ignore")

    Y = df[targets].astype(float).copy()

    # Очень простая стратегия заполнения пропусков: медианы по колонкам train (посчитаем позже)
    print(f'Finel X size {X.shape}')
    return df, X, Y

def time_train_test_split(df: pd.DataFrame, test_frac: float = 0.2):
    """
    Деление по времени: последние test_frac дат — в тест.
    Работает корректно для нескольких тикеров (сплит по общей оси времени).
    """
    df = df.sort_values("begin").reset_index(drop=True)
    uniq_dates = np.sort(df["begin"].dt.normalize().unique())
    split_idx = int(len(uniq_dates) * (1.0 - test_frac))
    split_date = uniq_dates[split_idx] if split_idx < len(uniq_dates) else uniq_dates[-1]
    train_mask = df["begin"].dt.normalize() < split_date
    test_mask = ~train_mask
    return train_mask, test_mask, pd.Timestamp(split_date)

def _parse_tickers_cell(x):
    if isinstance(x, list):
        return [str(t).strip().upper() for t in x]
    s = str(x)
    try:
        import ast
        v = ast.literal_eval(s)
        if isinstance(v, (list, tuple)):
            return [str(t).strip().upper() for t in v]
    except Exception:
        pass
    return [tok.strip().upper() for tok in s.replace("[","").replace("]","").replace("'","").split(",") if tok.strip()]

def _one_hot_label(lbl: str):
    lbl = str(lbl).lower()
    return (
        1.0 if lbl == "positive" else 0.0,
        1.0 if lbl == "negative" else 0.0,
        1.0 if lbl == "neutral"  else 0.0,
    )

def _build_news_tails(news: pd.DataFrame, N_NEWS: int = 10) -> tuple[dict, np.ndarray]:
    """
    Возвращает:
      tails_by_ticker: { 'GAZP': np.array(N_NEWS,7), ... }
      global_tail:     np.array(N_NEWS,7)
    Где каждая новость -> вектор [p_pos, p_neg, p_neu, sent_score, 1{pos},1{neg},1{neu}]
    """
    if news is None or len(news) == 0:
        # пусто — вернём нули
        return {}, np.zeros((N_NEWS, 7), dtype=np.float32)

    df = news.copy()
    if "publish_date" in df.columns:
        df["publish_date"] = pd.to_datetime(df["publish_date"], errors="coerce")
    else:
        df["publish_date"] = pd.Timestamp(0)

    df["tickers_list"] = df["tickers"].apply(_parse_tickers_cell)
    df = df.explode("tickers_list", ignore_index=True)
    df.rename(columns={"tickers_list":"ticker"}, inplace=True)
    df["ticker"] = df["ticker"].astype(str).str.upper()
    df = df.sort_values("publish_date")

    # числовые
    for c in ["p_positive","p_negative","p_neutral","sent_score"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
        else:
            df[c] = 0.0
    if "sent_label" not in df.columns:
        df["sent_label"] = "neutral"

    def news_rows_to_seq(dfi: pd.DataFrame, n: int) -> np.ndarray:
        tail = dfi.tail(n)
        rows = []
        for _, r in tail.iterrows():
            v = [
                float(r.get("p_positive", 0.0)),
                float(r.get("p_negative", 0.0)),
                float(r.get("p_neutral",  0.0)),
                float(r.get("sent_score", 0.0)),
            ]
            v += list(_one_hot_label(r.get("sent_label", "")))
            rows.append(v)
        seq = np.array(rows, dtype=np.float32)
        # паддинг сверху нулями до (n,7)
        if len(seq) < n:
            pad = np.zeros((n - len(seq), 7), dtype=np.float32)
            seq = np.vstack([pad, seq])
        return seq

    global_tail = news_rows_to_seq(df, N_NEWS)

    tails_by_ticker = {}
    for tkr, dfi in df.groupby("ticker"):
        tails_by_ticker[tkr] = news_rows_to_seq(dfi, N_NEWS)

    return tails_by_ticker, global_tail

def _ensure_numeric(X_train: pd.DataFrame, X_test: pd.DataFrame):
    """
    Оставляет только числовые столбцы, выравнивает порядок фич и заполняет NaN.
    """
    # 1) только числовые
    Xtr = X_train.select_dtypes(include=[np.number]).copy()
    Xte = X_test.select_dtypes(include=[np.number]).copy()

    # 2) выровнять колонки теста под трейн
    Xte = Xte.reindex(columns=Xtr.columns, fill_value=0.0)

    # 3) заполнить пропуски медианами train
    fillna_vals = Xtr.median(numeric_only=True)
    Xtr = Xtr.fillna(fillna_vals)
    Xte = Xte.fillna(fillna_vals)

    return Xtr, Xte

def get_pred(X_train, y_train, X_test, news=None, model_kind="gbr", random_state=42):
    """
    Мультивыходная регрессия с безопасным препроцессингом:
      - Численные: median-impute
      - Категориальные (object/category): OneHot (dense), handle_unknown='ignore'
      - Датовые колонки: дропаем
    """
    # --- базовая модель
    if model_kind == "gbr":
        base = GradientBoostingRegressor(random_state=random_state)
        need_dense_ohe = True
    elif model_kind == "rf":
        base = RandomForestRegressor(
            n_estimators=500, max_depth=None, n_jobs=-1, random_state=random_state
        )
        need_dense_ohe = True
    elif model_kind == "ridge":
        base = Ridge(alpha=1.0, random_state=random_state)
        need_dense_ohe = True  # можно и sparse, но для унификации оставим dense
    else:
        raise ValueError(f"Unknown model_kind: {model_kind}")

    est = MultiOutputRegressor(base)

    # --- превращаем в DataFrame/ndarray
    if not isinstance(X_train, pd.DataFrame):
        X_train = pd.DataFrame(X_train)
    if not isinstance(X_test, pd.DataFrame):
        X_test = pd.DataFrame(X_test)

    # --- типы признаков
    dt_cols = X_train.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
    # выбросим даты (если их надо — заранее преврати в числа)
    if dt_cols:
        X_train = X_train.drop(columns=dt_cols, errors="ignore")
        X_test  = X_test.drop(columns=dt_cols, errors="ignore")

    num_cols = X_train.select_dtypes(include=["number", "bool"]).columns.tolist()
    cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()

    # на тесте иногда появляются новые чисто NaN колонки/типы — синхронизируем
    for c in X_test.columns:
        if c not in num_cols and c not in cat_cols:
            if pd.api.types.is_numeric_dtype(X_test[c]) or pd.api.types.is_bool_dtype(X_test[c]):
                num_cols.append(c)
            elif pd.api.types.is_object_dtype(X_test[c]) or pd.api.types.is_categorical_dtype(X_test[c]):
                cat_cols.append(c)

    # — пайплайн препроцессинга
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=not need_dense_ohe and False)
    # выше оставили dense (sparse_output=False), т.к. GBR не принимает sparse

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", SimpleImputer(strategy="median"), num_cols),
            ("cat", Pipeline([("imputer", SimpleImputer(strategy="most_frequent")), ("ohe", ohe)]), cat_cols),
        ],
        remainder="drop",
        verbose_feature_names_out=False,
    )

    pipe = Pipeline(steps=[("prep", preprocessor), ("model", est)])

    # --- y в правильной форме (n_samples, n_targets)
    y_arr = np.asarray(y_train)
    if y_arr.ndim == 1:
        y_arr = y_arr.reshape(-1, 1)

    pipe.fit(X_train, y_arr)

    pred_train = pipe.predict(X_train)
    pred_test  = pipe.predict(X_test)
    return pred_train, pred_test

def get_pred_simple(X_train, y_train, X_test, model_kind="gbr", random_state=42):
    """
    Мультивыходная регрессия:
      - "gbr": GradientBoostingRegressor
      - "rf":  RandomForestRegressor
      - "ridge": Ridge
    """
    if model_kind == "gbr":
        base = GradientBoostingRegressor(random_state=random_state)
    elif model_kind == "rf":
        base = RandomForestRegressor(
            n_estimators=500, max_depth=None, n_jobs=-1, random_state=random_state
        )
    elif model_kind == "ridge":
        base = Ridge(alpha=1.0, random_state=random_state)
    else:
        raise ValueError(f"Unknown model_kind: {model_kind}")

    model = MultiOutputRegressor(base)
    model.fit(X_train, y_train)
    return model.predict(X_train), model.predict(X_test)

from sklearn.metrics import mean_absolute_error
import numpy as np
import pandas as pd

def evaluate(y_true: np.ndarray,
             y_pred: np.ndarray,
             tag: str = "TEST",
             p_pred: np.ndarray | None = None,
             prob_sensitivity: float = 12.0,
             print_table: bool = True):
    """
    Считаем метрики по каждому горизонту i и итоговый Score_i:
      Score_i = 0.7 * MAE_norm_i + 0.3 * Brier_norm_i + 0.1 * (1 / DA_i)

    Где:
      MAE_norm_i   = 1 - (MAE_i / MAE_base_i),       MAE_base_i = mean(|y_i|)
      Brier_norm_i = 1 - (Brier_i / Brier_base_i),   Brier_base_i = 0.25  (p_base=0.5)
      DA_i         = mean(sign(y_i) == sign(yhat_i))  (accuracy по знаку)

    Если p_pred не задано, вероятности роста берём как:
      p_pred = sigmoid(prob_sensitivity * y_pred)  (поэлементно)
    """
    eps = 1e-9
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    assert y_true.shape == y_pred.shape and y_true.ndim == 2, "y_true/y_pred должны быть (N, H)"

    N, H = y_true.shape

    # --- вероятности роста ---
    if p_pred is None:
        p_pred = 1.0 / (1.0 + np.exp(-prob_sensitivity * y_pred))
    else:
        p_pred = np.asarray(p_pred)
        if p_pred.shape != (N, H):
            raise ValueError("p_pred должен иметь форму (N, H)")

    y_up = (y_true > 0).astype(float)

    maes, maes_base, mae_norms = [], [], []
    briers, briers_base, brier_norms = [], [], []
    das, scores = [], []

    for i in range(H):
        yt = y_true[:, i]
        yp = y_pred[:, i]
        pp = p_pred[:, i]
        yi = y_up[:, i]

        # --- MAE и нормировка ---
        mae_i = mean_absolute_error(yt, yp)
        mae_base_i = np.mean(np.abs(yt))
        mae_norm_i = 1.0 - mae_i / (mae_base_i + eps)

        # --- Brier и нормировка ---
        brier_i = np.mean((yi - pp) ** 2)
        brier_base_i = 0.25  # при p_base = 0.5
        brier_norm_i = 1.0 - brier_i / (brier_base_i + eps)

        # --- Accuracy по знаку (DA) ---
        da_i = np.mean(np.sign(yt) == np.sign(yp))

        # --- Итоговый Score ---
        score_i = 0.7 * mae_norm_i + 0.3 * brier_norm_i + 0.1 * (1.0 / (da_i + eps))

        maes.append(mae_i); maes_base.append(mae_base_i); mae_norms.append(mae_norm_i)
        briers.append(brier_i); briers_base.append(brier_base_i); brier_norms.append(brier_norm_i)
        das.append(da_i); scores.append(score_i)

    maes = np.array(maes); maes_base = np.array(maes_base); mae_norms = np.array(mae_norms)
    briers = np.array(briers); briers_base = np.array(briers_base); brier_norms = np.array(brier_norms)
    das = np.array(das); scores = np.array(scores)

    if print_table:
        print(f"\n=== METRICS ({tag}) ===")
        header = "Hor |     MAE | MAE_base | MAE_norm |   Brier | Brier_base | Brier_norm |   DA   |    Score"
        print(header)
        for i in range(H):
            print(f"R_{i+1:>2} | {maes[i]:8.6f} | {maes_base[i]:8.6f} | {mae_norms[i]:8.4f} | "
                  f"{briers[i]:8.6f} | {briers_base[i]:11.6f} | {brier_norms[i]:10.4f} | "
                  f"{das[i]:6.3f} | {scores[i]:9.4f}")
        print("-" * len(header))
        print(f"AVG | {maes.mean():8.6f} | {maes_base.mean():8.6f} | {mae_norms.mean():8.4f} | "
              f"{briers.mean():8.6f} | {briers_base.mean():11.6f} | {brier_norms.mean():10.4f} | "
              f"{das.mean():6.3f} | {scores.mean():9.4f}")
        print(f"SUM Score over horizons: {scores.sum():.4f}")

    # Вернём средний итоговый score и DataFrame с подробностями (на всякий случай)
    details = pd.DataFrame({
        "horizon": [f"R_{i}" for i in range(1, H+1)],
        "MAE": maes, "MAE_base": maes_base, "MAE_norm": mae_norms,
        "Brier": briers, "Brier_base": briers_base, "Brier_norm": brier_norms,
        "DA": das, "Score": scores
    })
    return scores.mean(), details

def generate_submissions(
    candles_path: str = "candles.csv",
    candles2_path: str = "candles_2.csv",
    news_path: str | None = "news.csv",
    news2_path: str | None = "news_2.csv",
    max_horizon: int = 20,
    submission_csv: str = "submissions.csv",
):
    """
    1) Обучаемся на candles.csv (+news.csv, если есть)
    2) Предсказываем для ПОСЛЕДНЕЙ свечи каждого тикера из candles_2.csv (+news_2.csv, если есть)
    3) Сохраняем CSV формата: ticker,p1..p20
    """
    # ---------- load ----------
    df_tr = pd.read_csv(candles_path)
    df_te = pd.read_csv(candles2_path)

    for d in (df_tr, df_te):
        d["begin"] = pd.to_datetime(d["begin"], errors="coerce")
        d["ticker"] = d["ticker"].astype(str).str.upper()

    # ---------- train targets & X/Y ----------
    df_tr_lbl = make_targets(df_tr, max_horizon=max_horizon)
    df_train, X_train, Y_train = build_xy(df_tr_lbl, max_horizon=max_horizon)

    # добавим тикер в X (чтобы get_pred мог приклеить новости по тикеру)
    X_train_w = X_train.copy()
    if "ticker" not in X_train_w.columns and "ticker" in df_train.columns:
        X_train_w["ticker"] = df_train["ticker"].values

    # simple imputing
    fillna_vals = X_train_w.select_dtypes(include=[np.number]).median(numeric_only=True)
    X_train_w[X_train_w.select_dtypes(include=[np.number]).columns] = (
        X_train_w.select_dtypes(include=[np.number]).fillna(fillna_vals)
    )

    # ---------- test rows: одна (последняя) свеча на тикер ----------
    te_sorted = df_te.sort_values(["ticker", "begin"])
    te_last = te_sorted.groupby("ticker").tail(1).copy()

    X_test = te_last.select_dtypes(include=[np.number]).copy()
    X_test = X_test.drop(columns=["Unnamed: 0"], errors="ignore")
    X_test["ticker"] = te_last["ticker"].values

    # привести набор колонок X_test к train
    for c in X_train_w.columns:
        if c not in X_test.columns and c != "ticker":
            X_test[c] = 0.0
    extra_in_test = [c for c in X_test.columns if c not in X_train_w.columns]
    if extra_in_test:
        X_test = X_test.drop(columns=extra_in_test, errors="ignore")
    X_test = X_test.reindex(columns=X_train_w.columns)

    num_cols_te = X_test.select_dtypes(include=[np.number]).columns
    X_test[num_cols_te] = X_test[num_cols_te].fillna(fillna_vals)

    # ---------- новости ----------
    if news_path is not None:
        try:
            news_tr = pd.read_csv(news_path)
        except Exception:
            news_tr = pd.DataFrame({"tickers": [], "publish_date": []})
    else:
        news_tr = pd.DataFrame({"tickers": [], "publish_date": []})

    if news2_path is not None:
        try:
            news_te = pd.read_csv(news2_path)
        except Exception:
            news_te = pd.DataFrame({"tickers": [], "publish_date": []})
    else:
        news_te = pd.DataFrame({"tickers": [], "publish_date": []})

    news_all = pd.concat([news_tr, news_te], ignore_index=True)

    # ---------- модель и предсказания ----------
    use_simple = (len(news_all) == 0) and callable(globals().get("get_pred_simple"))

    if use_simple:
        pred_train, pred_test = get_pred_simple(X_train_w, Y_train, X_test)
    else:
        pred_train, pred_test = get_pred(X_train_w, Y_train, X_test, news_all)

    # ---------- EVALUATE (на обучении) ----------
    # метрики по всем горизонтам для тренировки
    evaluate(Y_train.values, pred_train, tag="TRAIN")

    # ---------- сохранить submissions ----------
    sub = pd.DataFrame({"ticker": te_last["ticker"].values})
    for i in range(1, max_horizon + 1):
        sub[f"p{i}"] = pred_test[:, i - 1]
    sub = sub.sort_values("ticker").reset_index(drop=True)

    Path(submission_csv).parent.mkdir(parents=True, exist_ok=True)
    sub.to_csv(submission_csv, index=False)
    print(f"✅ Saved {submission_csv} (rows: {len(sub)})")
    return sub

_ = generate_submissions(
    candles_path="candles_features.csv",
    candles2_path="candles_2_features.csv",
    news_path="fast_train_news_with_sent.csv",        # можно None
    news2_path="fast_test_news_with_sent.csv",     # можно None
    max_horizon=20,
    submission_csv="submissions.csv",
)