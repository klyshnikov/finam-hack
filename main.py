# -*- coding: utf-8 -*-
"""mvp_forecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yexc3XV8u600ggjHm2BBdmDmeQ-0TClV
"""

import numpy as np
import pandas as pd
from pathlib import Path

import re, ast
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD, NMF

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

"""### 1 —á–∞—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è - –¥–æ–±–∞–≤–∏—Ç—å —Ç–∏–Ω–∫–µ—Ä—ã –≤ –Ω–æ–≤–æ—Å—Ç–∏"""

candles = pd.read_csv("candles.csv")


display(candles.head())


tickers = candles["ticker"].unique().tolist()


ticker_patterns = {
    "AFLT": [r"(?:^|[^–∞-—èa-z—ë])–∞—ç—Ä–æ—Ñ–ª–æ—Ç[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])aeroflot[a-z]*"],
    "ALRS": [r"(?:^|[^–∞-—èa-z—ë])–∞–ª—Ä–æ—Å[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])alrosa[a-z]*"],
    "CHMF": [r"(?:^|[^–∞-—èa-z—ë])—Å–µ–≤–µ—Ä—Å—Ç–∞–ª[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])severstal[a-z]*", r"(?:^|[^–∞-—èa-z—ë])chmf"],
    "GAZP": [r"(?:^|[^–∞-—èa-z—ë])–≥–∞–∑–ø—Ä–æ–º[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])gazprom[a-z]*"],
    "GMKN": [
        r"(?:^|[^–∞-—èa-z—ë])–Ω–æ—Ä–Ω–∏–∫–µ–ª[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])–Ω–æ—Ä–∏–ª—å—Å–∫[–∞-—è—ë]*\s–Ω–∏–∫–µ–ª[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])nornickel[a-z]*",
        r"(?:^|[^–∞-—èa-z—ë])gmkn"
    ],
    "LKOH": [r"(?:^|[^–∞-—èa-z—ë])–ª—É–∫–æ–π–ª[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])lukoil[a-z]*"],
    "MAGN": [
        r"(?:^|[^–∞-—èa-z—ë])–º–º–∫",
        r"(?:^|[^–∞-—èa-z—ë])–º–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫[–∞-—è—ë]*\s–º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])magn[a-z]*"
    ],
    "MGNT": [r"(?:^|[^–∞-—èa-z—ë])–º–∞–≥–Ω–∏—Ç[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])magnit[a-z]*"],
    "MOEX": [r"(?:^|[^–∞-—èa-z—ë])–º–æ—Å–∫–æ–≤—Å–∫[–∞-—è—ë]*\s–±–∏—Ä–∂[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])moex[a-z]*"],
    "MTSS": [r"(?:^|[^–∞-—èa-z—ë])–º—Ç—Å", r"(?:^|[^–∞-—èa-z—ë])mts", r"(?:^|[^–∞-—èa-z—ë])mobile\s+telesystems"],
    "NVTK": [r"(?:^|[^–∞-—èa-z—ë])–Ω–æ–≤–∞—Ç[–µ—ç]–∫[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])novatek[a-z]*"],
    "PHOR": [r"(?:^|[^–∞-—èa-z—ë])—Ñ–æ—Å–∞–≥—Ä[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])phosagro[a-z]*"],
    "PLZL": [
        r"(?:^|[^–∞-—èa-z—ë])–ø–æ–ª—é—Å[–∞-—è—ë]*\s–∑–æ–ª–æ—Ç[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])polyus[a-z]*",
        r"(?:^|[^–∞-—èa-z—ë])plzl"
    ],
    "ROSN": [r"(?:^|[^–∞-—èa-z—ë])—Ä–æ—Å–Ω–µ—Ñ—Ç[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])rosneft[a-z]*"],
    "RUAL": [r"(?:^|[^–∞-—èa-z—ë])—Ä—É—Å–∞–ª[–∞-—è—ë]*", r"(?:^|[^–∞-—èa-z—ë])rusal[a-z]*"],
    "SBER": [
        r"(?:^|[^–∞-—èa-z—ë])—Å–±–µ—Ä–±–∞–Ω–∫[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])—Å–±–µ—Ä[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])sber[a-z]*",
        r"(?:^|[^–∞-—èa-z—ë])sberbank[a-z]*"
    ],
    "SIBN": [
        r"(?:^|[^–∞-—èa-z—ë])–≥–∞–∑–ø—Ä–æ–º\s+–Ω–µ—Ñ—Ç[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])gazprom\s+neft[a-z]*"
    ],
    "T": [
        r"(?:^|[^–∞-—èa-z—ë])—Ç-?–±–∞–Ω–∫[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])—Ç–∏–Ω—å–∫–æ—Ñ—Ñ[–∞-—è—ë]*",
        r"(?:^|[^–∞-—èa-z—ë])tinkoff[a-z]*",
        r"(?:^|[^–∞-—èa-z—ë])t-?bank[a-z]*"
    ],
    "VTBR": [r"(?:^|[^–∞-—èa-z—ë])–≤—Ç–±", r"(?:^|[^–∞-—èa-z—ë])vtb[a-z]*"],
}

# ===== –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–∏–∫–µ—Ä–æ–≤ =====
def extract_tickers(text, mapping=ticker_patterns):
    text = str(text).lower()
    found = []
    for ticker, patterns in mapping.items():
        if any(re.search(p, text) for p in patterns):
            found.append(ticker)
    return list(set(found)) if found else None

# ===== –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º =====
train_news_path = "news.csv"
test_news_path = "news_2.csv"

# ===== –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ =====
train_news = pd.read_csv(train_news_path)
test_news = pd.read_csv(test_news_path)

# ===== –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É tickers =====
def add_tickers_column(df):
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º title + publication
    if "title" in df.columns and "publication" in df.columns:
        text_data = df["title"].fillna("") + " " + df["publication"].fillna("")
    elif "title" in df.columns:
        text_data = df["title"].fillna("")
    else:
        text_data = df.astype(str).agg(" ".join, axis=1)

    df["tickers"] = text_data.apply(extract_tickers)
    return df

train_news = add_tickers_column(train_news)
test_news = add_tickers_column(test_news)

# ===== –°–æ—Ö—Ä–∞–Ω—è–µ–º =====
out_dir = Path("data/processed/")
out_dir.mkdir(parents=True, exist_ok=True)

train_out = out_dir / "train_news_with_tickers.csv"
test_out = out_dir / "test_news_with_tickers.csv"

train_news.to_csv(train_out, index=False)
test_news.to_csv(test_out, index=False)

print("‚úÖ –§–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")

"""### 2 —á–∞—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è - —Ä–∞–∑–º–µ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""

from __future__ import annotations

import os
import math
from dataclasses import dataclass, replace
from typing import List, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from tqdm.auto import tqdm
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification,
)

torch.backends.cuda.matmul.allow_tf32 = True
try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

__all__ = [
    "FinBertConfig",
    "RUN_FLAGS",
    "process_news_with_config",
    "run_with_flags",
    "detect_device",
]

@dataclass
class FinBertConfig:
    input_csv: str
    output_csv: str
    no_translate: bool = False            # True => –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç
    batch_size: int = 64
    max_seq_len: int = 256
    max_new_tokens: int = 96
    truncate_publication: Optional[int] = 300  # 0/None => –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å publication
    use_ctranslate2: bool = False         # True => –Ω—É–∂–µ–Ω ctranslate2 + –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    ctranslate2_dir: Optional[str] = None
    title_only: bool = False              # True => –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ title

# –ø—Ä–∞–≤—å –ø—É—Ç–∏/–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ —Å–≤–æ–π –ø—Ä–æ–µ–∫—Ç
RUN_FLAGS = FinBertConfig(
    input_csv="train_news_with_tickers.csv",
    output_csv="fast_train_news_with_sent.csv",
    no_translate=False,
    batch_size=64,
    max_seq_len=256,
    max_new_tokens=96,
    truncate_publication=300,
    use_ctranslate2=False,
    ctranslate2_dir=None,
    title_only=False,
)

RUN_FLAGS_TEST = FinBertConfig(
    input_csv="test_news_with_tickers.csv",
    output_csv="fast_test_news_with_sent.csv",
    no_translate=False,
    batch_size=64,
    max_seq_len=256,
    max_new_tokens=96,
    truncate_publication=300,
    use_ctranslate2=False,
    ctranslate2_dir=None,
    title_only=False,
)

# ------------------ –£—Ç–∏–ª–∏—Ç—ã ------------------
def detect_device():
    dev = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[Device] torch={torch.__version__} | device={dev} | cuda={getattr(torch.version, 'cuda', None)}")
    if dev == "cuda":
        try:
            print(f"[GPU] {torch.cuda.get_device_name(0)} | mem={(torch.cuda.get_device_properties(0).total_memory/1e9):.1f} GB")
        except Exception as e:
            print(f"[GPU warn] {e}")
    return dev

def build_text(title, body, truncate_publication: Optional[int] = None):
    t = (str(title) if pd.notna(title) else "").strip()
    b = (str(body) if pd.notna(body) else "").strip()
    if truncate_publication and isinstance(truncate_publication, int) and truncate_publication > 0:
        b = b[:truncate_publication]
    if t and b:
        return f"{t}. {b}"
    return t or b

def safe_read_csv(path, encodings=("utf-8", "utf-8-sig", "cp1251")) -> pd.DataFrame:
    last_err = None
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception as e:
            last_err = e
    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV '{path}'. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_err}")

# ------------------ –ü–µ—Ä–µ–≤–æ–¥ RU‚ÜíEN ------------------
def load_translation_model(device: str, model_name="Helsinki-NLP/opus-mt-ru-en"):
    tok = AutoTokenizer.from_pretrained(model_name)
    kwargs = {}
    if device == "cuda":
        kwargs["torch_dtype"] = torch.float16
    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name, **kwargs).to(device)
    if device == "cuda":
        mdl.half()
    else:
        from torch.quantization import quantize_dynamic
        mdl = quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8)
    mdl.eval()
    print("[OK] Translation model loaded:", model_name)
    print("Translator on:", next(mdl.parameters()).device)
    return tok, mdl

def translate_texts(texts: List[str], tok, mdl, device: str,
                    batch_size=32, max_len=512, max_new_tokens=128) -> List[str]:
    res = []
    total_batches = math.ceil(len(texts) / batch_size)
    for i in tqdm(range(0, len(texts), batch_size), total=total_batches, desc="Translate RU‚ÜíEN (HF greedy)"):
        batch = texts[i:i+batch_size]
        inputs = tok(batch, return_tensors="pt", truncation=True, padding=True,
                     max_length=max_len, pad_to_multiple_of=8).to(device)
        with torch.inference_mode():
            gen = mdl.generate(
                **inputs,
                num_beams=1,
                do_sample=False,
                use_cache=True,
                max_new_tokens=max_new_tokens,
            )
        out = tok.batch_decode(gen, skip_special_tokens=True)
        res.extend(out)
    return res

# --- CTranslate2 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ---
def translate_ct2(texts: List[str], model_dir: str, device: str,
                  batch_size=128, max_len=256, max_new_tokens=96) -> List[str]:
    try:
        import ctranslate2
        import sentencepiece as spm
    except Exception as e:
        raise RuntimeError("–î–ª—è use_ctranslate2 –Ω—É–∂–Ω—ã –ø–∞–∫–µ—Ç—ã: ctranslate2 –∏ sentencepiece") from e

    sp_src = spm.SentencePieceProcessor()
    sp_tgt = spm.SentencePieceProcessor()
    sp_src.load(os.path.join(model_dir, "source.spm"))
    sp_tgt.load(os.path.join(model_dir, "target.spm"))

    compute_type = "float16" if device == "cuda" else "int8"
    translator = ctranslate2.Translator(model_dir, device=device, compute_type=compute_type)

    res = []
    total_batches = math.ceil(len(texts) / batch_size)
    for i in tqdm(range(0, len(texts), batch_size), total=total_batches, desc=f"Translate RU‚ÜíEN (ct2 {compute_type})"):
        batch = texts[i:i+batch_size]
        src_tokens = [sp_src.encode(t, out_type=str)[:max_len] for t in batch]
        out = translator.translate_batch(src_tokens, beam_size=1, max_decoding_length=max_new_tokens)
        res.extend(sp_tgt.decode_pieces(o.hypotheses[0]) for o in out)
    return res

# ------------------ –°–µ–Ω—Ç–∏–º–µ–Ω—Ç ------------------
def load_finbert(device: str, model_name="ProsusAI/finbert"):
    tok = AutoTokenizer.from_pretrained(model_name)
    kwargs = {}
    if device == "cuda":
        kwargs["torch_dtype"] = torch.float16
    mdl = AutoModelForSequenceClassification.from_pretrained(model_name, **kwargs).to(device)
    if device == "cuda":
        mdl.half()
    else:
        from torch.quantization import quantize_dynamic
        mdl = quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8)
    mdl.eval()
    try:
        from optimum.bettertransformer import BetterTransformer
        mdl = BetterTransformer.transform(mdl, keep_original_model=False)
        print("[BT] BetterTransformer enabled")
    except Exception:
        pass
    # torch.compile (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    try:
        if device == "cuda":
            mdl = torch.compile(mdl, mode="max-autotune")
            print("[Compile] torch.compile enabled")
    except Exception:
        pass

    id2label = {i: mdl.config.id2label[i] for i in range(mdl.config.num_labels)}
    label_names = [id2label[i].lower() for i in range(mdl.config.num_labels)]
    print("[OK] FinBERT loaded:", model_name, "| labels:", label_names)
    print("FinBERT on:", next(mdl.parameters()).device)
    return tok, mdl, label_names

def load_multilingual_sentiment(device: str, model_name="cardiffnlp/twitter-xlm-roberta-base-sentiment"):
    tok = AutoTokenizer.from_pretrained(model_name)
    kwargs = {}
    if device == "cuda":
        kwargs["torch_dtype"] = torch.float16
    mdl = AutoModelForSequenceClassification.from_pretrained(model_name, **kwargs).to(device)
    if device == "cuda":
        mdl.half()
    else:
        from torch.quantization import quantize_dynamic
        mdl = quantize_dynamic(mdl, {nn.Linear}, dtype=torch.qint8)
    mdl.eval()
    try:
        from optimum.bettertransformer import BetterTransformer
        mdl = BetterTransformer.transform(mdl, keep_original_model=False)
        print("[BT] BetterTransformer enabled (multi)")
    except Exception:
        pass
    try:
        if device == "cuda":
            mdl = torch.compile(mdl, mode="max-autotune")
            print("[Compile] torch.compile enabled (multi)")
    except Exception:
        pass
    id2label = {i: mdl.config.id2label[i] for i in range(mdl.config.num_labels)}
    label_names = [id2label[i].lower() for i in range(mdl.config.num_labels)]
    print("[OK] Multilingual sentiment loaded:", model_name, "| labels:", label_names)
    print("Sentiment model on:", next(mdl.parameters()).device)
    return tok, mdl, label_names

def run_text_classifier(texts: List[str], tok, mdl, device: str,
                        batch_size=64, max_len=512, desc="Classifier") -> np.ndarray:
    total_batches = math.ceil(len(texts) / batch_size)
    out_chunks = []
    for start in tqdm(range(0, len(texts), batch_size), total=total_batches, desc=desc):
        batch = texts[start:start+batch_size]
        enc = tok(batch, return_tensors="pt", truncation=True, padding=True,
                  max_length=max_len, pad_to_multiple_of=8).to(device)
        with torch.inference_mode():
            logits = mdl(**enc).logits
            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()
        out_chunks.append(probs)
    return np.vstack(out_chunks) if out_chunks else np.empty((0, 0))

def process_news_with_config(cfg: FinBertConfig) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–æ–Ω—Ñ–∏–≥—É cfg. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç CSV (+ .partial.csv).
    """
    device = detect_device()

    # 1) —á–∏—Ç–∞–µ–º CSV
    df = safe_read_csv(cfg.input_csv)
    required_cols = {"publish_date", "title", "publication", "tickers"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"–í CSV –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã: {missing}")

    # 2) —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
    if cfg.title_only:
        df["__text_ru"] = df["title"].fillna("").astype(str)
    else:
        df["__text_ru"] = [
            build_text(t, p, truncate_publication=cfg.truncate_publication)
            for t, p in zip(df["title"], df["publication"])
        ]

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    text_codes, unique_texts = pd.factorize(df["__text_ru"], sort=False)
    unique_texts = list(unique_texts)

    # 3) –ü–µ—Ä–µ–≤–æ–¥ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
    if not cfg.no_translate:
        if cfg.use_ctranslate2:
            if not cfg.ctranslate2_dir or not os.path.isdir(cfg.ctranslate2_dir):
                raise ValueError("use_ctranslate2=True, –Ω–æ ctranslate2_dir –Ω–µ –∑–∞–¥–∞–Ω –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            unique_texts_en = translate_ct2(
                unique_texts, cfg.ctranslate2_dir, device,
                batch_size=max(64, cfg.batch_size),
                max_len=cfg.max_seq_len,
                max_new_tokens=cfg.max_new_tokens,
            )
        else:
            trans_tok, trans_mdl = load_translation_model(device)
            unique_texts_en = translate_texts(
                unique_texts, trans_tok, trans_mdl, device,
                batch_size=cfg.batch_size,
                max_len=cfg.max_seq_len,
                max_new_tokens=cfg.max_new_tokens,
            )
        texts_en = [unique_texts_en[c] for c in text_codes]
    else:
        texts_en = None

    # 4) –°–µ–Ω—Ç–∏–º–µ–Ω—Ç
    if not cfg.no_translate:
        fb_tok, fb_mdl, label_names = load_finbert(device)
        probs_unique = run_text_classifier(
            unique_texts_en, fb_tok, fb_mdl, device,
            batch_size=cfg.batch_size, max_len=cfg.max_seq_len, desc="FinBERT (EN)"
        )
        probs_out = probs_unique[text_codes]
    else:
        ml_tok, ml_mdl, label_names = load_multilingual_sentiment(device)
        probs_unique = run_text_classifier(
            unique_texts, ml_tok, ml_mdl, device,
            batch_size=cfg.batch_size, max_len=cfg.max_seq_len, desc="Multilingual sentiment (RU)"
        )
        probs_out = probs_unique[text_codes]

    # 5) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    out_df = df.copy()
    for j, name in enumerate(label_names):
        out_df[f"p_{name.lower()}"] = probs_out[:, j]

    ln = [n.lower() for n in label_names]
    idx_pos = ln.index("positive") if "positive" in ln else None
    idx_neg = ln.index("negative") if "negative" in ln else None

    out_df["sent_label"] = [label_names[int(i)] for i in np.argmax(probs_out, axis=1)]
    if idx_pos is not None and idx_neg is not None:
        out_df["sent_score"] = probs_out[:, idx_pos] - probs_out[:, idx_neg]
    else:
        out_df["sent_score"] = np.nan

    out_df.drop(columns=["__text_ru"], inplace=True, errors="ignore")
    out_df.to_csv(cfg.output_csv + ".partial.csv", index=False)
    out_df.to_csv(cfg.output_csv, index=False)
    print(f"[OK] Saved: {cfg.output_csv}")
    print(f"[Note] Partial file: {cfg.output_csv}.partial.csv (–æ—Å—Ç–∞–≤–ª–µ–Ω)")
    return out_df

def run_with_flags(**overrides) -> pd.DataFrame:
    """
    –ó–∞–ø—É—Å–∫ —Å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–º–∏ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –¥–µ—Ñ–æ–ª—Ç–æ–≤:
        run_with_flags(output_csv="...", no_translate=True)
    (–Ω–µ CLI, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –≤ Python-–∫–æ–¥–µ)
    """
    cfg = replace(RUN_FLAGS, **overrides)
    return process_news_with_config(cfg)

print("[RUN] finbert.py –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ RUN_FLAGS:")
print(RUN_FLAGS)
process_news_with_config(RUN_FLAGS)
process_news_with_config(RUN_FLAGS_TEST)

"""### 3 —á–∞—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è - —Ä–∞–∑–º–µ—Ç–∏–º candles."""

def _pick_device(device: str = "auto"):
    import importlib
    import numpy as _np
    import pandas as _pd
    from scipy import sparse as _sp

    dev = "cpu"
    mods = {}

    if device in ("auto", "gpu"):
        try:
            _cp = importlib.import_module("cupy")
            _cudf = importlib.import_module("cudf")
            _cuml = importlib.import_module("cuml")
            _cupyx_sp = importlib.import_module("cupyx.scipy.sparse")

            tfidf_cls = importlib.import_module("cuml.feature_extraction.text").TfidfVectorizer
            svd_cls = importlib.import_module("cuml.decomposition").TruncatedSVD

            from sklearn.decomposition import NMF as _skNMF

            def _to_cpu_sparse(x):
                return x.get()
            def _to_cpu(x):
                return _cp.asnumpy(x)

            def _to_cudf_series(ps):
                return _cudf.Series(ps.astype(str).fillna(""))

            dev = "gpu"
            mods.update(dict(
                xp=_cp, pd=_pd, sp=_cupyx_sp,
                tfidf_cls=tfidf_cls, svd_cls=svd_cls, nmf_cls=_skNMF,
                to_cpu_sparse=_to_cpu_sparse, to_cpu=_to_cpu, to_cudf_series=_to_cudf_series,
                cudf=_cudf, cuml=_cuml
            ))
        except Exception:
            pass

    if dev == "cpu":
        from sklearn.feature_extraction.text import TfidfVectorizer as _skTFIDF
        from sklearn.decomposition import TruncatedSVD as _skSVD, NMF as _skNMF
        def _to_cpu_sparse(x): return x
        def _to_cpu(x): return x
        def _to_cudf_series(ps): return ps

        mods.update(dict(
            xp=_np, pd=_pd, sp=_sp,
            tfidf_cls=_skTFIDF, svd_cls=_skSVD, nmf_cls=_skNMF,
            to_cpu_sparse=_to_cpu_sparse, to_cpu=_to_cpu, to_cudf_series=_to_cudf_series
        ))
    return dev, mods

def hash_embed_publication(pub: str, dim: int = 8, seed: int = 42):
    rnd = np.random.RandomState(abs(hash((pub, seed))) % (2**32))
    v = rnd.normal(size=dim).astype(np.float32)
    v /= (np.linalg.norm(v) + 1e-12)
    return v

def softmax(x, xp=None):
    if xp is None:
        import numpy as np
        xp = np
    x = xp.asarray(x, dtype=xp.float32)
    if x.size:
        x = x - x.max()
    ex = xp.exp(x)
    return ex / (ex.sum() + 1e-12)

def transformer_like_pool(embs, times, now_ts, tau_days: float = 10.0, time_bias: float = 1.0, xp=None):
    import pandas as pd
    if xp is None:
        import numpy as _np
        xp = _np

    embs = xp.asarray(embs, dtype=xp.float32)
    if embs.ndim == 1:
        embs = embs.reshape(1, -1)
    if embs.shape[0] == 0:
        return xp.zeros((embs.shape[1],), dtype=xp.float32)

    D = embs.shape[1]
    q = embs.mean(axis=0).astype(xp.float32)
    scores = (embs @ q) / (xp.sqrt(D) + 1e-9)

    # –¥–∞—Ç—ã —Å—á–∏—Ç–∞–µ–º –Ω–∞ CPU, –∑–∞—Ç–µ–º –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤ xp
    times_ns = pd.to_datetime(times).values.astype('datetime64[ns]')
    now_ns   = pd.Timestamp(now_ts).to_datetime64().astype('datetime64[ns]')
    dt_days_cpu = ((now_ns - times_ns).astype('timedelta64[s]').astype('float64') / 86400.0)
    dt_days_cpu = dt_days_cpu.clip(min=0.0)

    dt_days = xp.asarray(dt_days_cpu, dtype=xp.float32)
    weights = softmax(scores + time_bias * (-dt_days / max(1e-6, tau_days)), xp=xp)
    ctx = (weights[:, None] * embs).sum(axis=0).astype(xp.float32)
    return ctx

def _norm_ticker(x: str) -> str:
    return re.sub(r"\s+", "", str(x).upper())

def _parse_tickers_cell(x):
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return []
    if isinstance(x, (list, tuple, set)):
        seq = list(x)
    else:
        s = str(x).strip()
        if not s:
            return []
        try:
            v = ast.literal_eval(s)
            if isinstance(v, (list, tuple, set)):
                seq = list(v)
            else:
                seq = [s]
        except Exception:
            seq = re.split(r"[,\|\s;]+", s.strip("[](){}'\" "))
    return [_norm_ticker(t) for t in seq if str(t).strip()]

class NewsVectorizer:
    """TF-IDF ‚Üí SVD(text_dim) + NMF(n_topics). GPU if available (RAPIDS); NMF stays on CPU if needed."""
    def __init__(self, text_dim=128, n_topics=16, max_features=30000, random_state=42, device: str = "auto"):
        self.text_dim = text_dim
        self.n_topics = n_topics
        self.max_features = max_features
        self.random_state = random_state
        self.device, self.mods = _pick_device(device)
        self.vec = None
        self.svd = None
        self.nmf = None

    def fit(self, news_texts: pd.Series):
        pd = self.mods["pd"]
        tfidf_cls = self.mods["tfidf_cls"]
        nmf_cls = self.mods["nmf_cls"]
        to_cpu_sparse = self.mods["to_cpu_sparse"]
        to_cudf_series = self.mods["to_cudf_series"]

        from sklearn.decomposition import TruncatedSVD as _skSVD

        if self.device == "gpu":
            texts_gpu = to_cudf_series(news_texts.astype(str).fillna(''))
            self.vec = tfidf_cls(max_features=self.max_features, ngram_range=(1,2), min_df=2)
            X_gpu = self.vec.fit_transform(texts_gpu)
            X_cpu = to_cpu_sparse(X_gpu)

            self.svd = _skSVD(n_components=self.text_dim, random_state=self.random_state)
            self.svd.fit(X_cpu)

            if self.n_topics and self.n_topics > 0:
                self.nmf = nmf_cls(n_components=self.n_topics, init='nndsvda',
                                random_state=self.random_state, max_iter=300)
                self.nmf.fit(X_cpu)
        else:
            from sklearn.feature_extraction.text import TfidfVectorizer as _skTFIDF
            from sklearn.decomposition import TruncatedSVD as _skSVD
            self.vec = _skTFIDF(max_features=self.max_features, ngram_range=(1,2), min_df=2)
            X = self.vec.fit_transform(news_texts.astype(str).fillna(''))
            self.svd = _skSVD(n_components=self.text_dim, random_state=self.random_state)
            self.svd.fit(X)
            if self.n_topics and self.n_topics > 0:
                self.nmf = nmf_cls(n_components=self.n_topics, init='nndsvda',
                                random_state=self.random_state, max_iter=300)
                self.nmf.fit(X)
        return self


    def transform(self, texts: pd.Series):
        import numpy as np
        to_cpu_sparse = self.mods["to_cpu_sparse"]
        to_cudf_series = self.mods["to_cudf_series"]

        if self.device == "gpu":
            texts_gpu = to_cudf_series(texts.astype(str).fillna(''))
            X_gpu = self.vec.transform(texts_gpu)
            X_cpu = to_cpu_sparse(X_gpu)

            xs = self.svd.transform(X_cpu).astype(np.float32)
            if self.nmf is not None:
                xt = self.nmf.transform(X_cpu).astype(np.float32)
                xt /= (xt.sum(axis=1, keepdims=True) + 1e-9)
            else:
                xt = np.zeros((xs.shape[0], self.n_topics), dtype=np.float32)
            return xs, xt
        else:
            X = self.vec.transform(texts.astype(str).fillna(''))
            xs = self.svd.transform(X).astype(np.float32)
            if self.nmf is not None:
                xt = self.nmf.transform(X).astype(np.float32)
                xt /= (xt.sum(axis=1, keepdims=True) + 1e-9)
            else:
                xt = np.zeros((xs.shape[0], self.n_topics), dtype=np.float32)
            return xs, xt

def make_extra_news_features(day_df: pd.DataFrame, last_60d_df: pd.DataFrame | None, n_topics: int):
    feats = {}
    cnt = len(day_df)
    feats['news_count_day'] = int(cnt)

    if cnt == 0:
        feats['dup_ratio'] = 0.0
        feats['title_len_mean'] = 0.0
        feats['title_len_std']  = 0.0
        feats['publication_len_mean'] = 0.0
        feats['publication_len_std']  = 0.0
        feats['top_topic_idx'] = -1
        feats['top_topic_strength'] = 0.0
        for i in range(n_topics):
            feats[f'topic_mean_{i}'] = 0.0
        feats['news_spike_rough'] = 0.0
        return feats

    txt = (day_df['title'].astype(str).str.lower().str.strip() + ' ' +
           day_df['publication'].astype(str).str.lower().str.strip()).str.replace(r'\s+', ' ', regex=True)
    uniq = int(txt.nunique())
    feats['dup_ratio'] = float(1.0 - uniq / max(1, cnt))

    tlen = day_df['title'].astype(str).str.len()
    plen = day_df['publication'].astype(str).str.len()
    feats['title_len_mean'] = float(tlen.mean())
    feats['title_len_std']  = float(tlen.std() or 0.0)
    feats['publication_len_mean'] = float(plen.mean())
    feats['publication_len_std']  = float(plen.std() or 0.0)

    topic_mat = np.stack(day_df['topic_vec'].values, axis=0)  # (cnt, K)
    topic_mean = topic_mat.mean(axis=0)
    top_idx = int(np.argmax(topic_mean))
    feats['top_topic_idx'] = top_idx
    feats['top_topic_strength'] = float(topic_mean[top_idx])
    for i, v in enumerate(topic_mean):
        feats[f'topic_mean_{i}'] = float(v)

    if last_60d_df is not None and len(last_60d_df):
        days = last_60d_df['publish_date'].dt.normalize().nunique()
        avg_per_day = float(len(last_60d_df)) / max(1, int(days))
        feats['news_spike_rough'] = float(cnt / (avg_per_day + 1e-9))
    else:
        feats['news_spike_rough'] = float(cnt)
    return feats

def add_news_context_simple(
    candles_csv: str,
    news_csv: str,
    out_csv: str = None,
    text_dim: int = 128,
    n_topics: int = 16,
    max_news_per_sample: int = 128,
    tau_days: float = 10.0,
    add_aditional_emb: bool = True,
    add_context_emb: bool = True,
    device: str = "auto"
):
    """
    –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω GPU –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω RAPIDS (cuDF/cuML), TF-IDF –∏ SVD –ø–æ–π–¥—É—Ç –Ω–∞ GPU.
    NMF ‚Äî –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ CPU (–ø—Ä–æ–∑—Ä–∞—á–Ω–æ), –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
    """
    import numpy as np
    import pandas as pd
    from pathlib import Path

    dev, mods = _pick_device(device)
    xp = mods["xp"]

    # 1) —Å–≤–µ—á–∏
    candles = pd.read_csv(candles_csv)
    candles['begin'] = pd.to_datetime(candles['begin'])
    candles['date'] = candles['begin'].dt.normalize()
    candles['ticker_norm'] = candles['ticker'].apply(_norm_ticker)

    # 2) –Ω–æ–≤–æ—Å—Ç–∏
    news = pd.read_csv(news_csv)
    news['publish_date'] = pd.to_datetime(news['publish_date'])
    news['tickers_list'] = news['tickers'].apply(_parse_tickers_cell)

    out_parts = [candles]

    # =========================
    # A) –ö–û–ù–¢–ï–ö–°–¢–ù–´–ï –≠–ú–ë–ï–î–î–ò–ù–ì–ò (–∏–∑ title –ò publication)
    # =========================
    if add_context_emb:
        vec_title = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['title'].astype(str).fillna('')
        )
        vec_pub   = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['publication'].astype(str).fillna('')
        )

        xs_t, xt_t = vec_title.transform(news['title'].astype(str).fillna(''))
        xs_p, xt_p = vec_pub.transform(news['publication'].astype(str).fillna(''))

        news_ctx = news[['publish_date','tickers_list']].copy()
        emb_np = np.concatenate([xs_t, xt_t, xs_p, xt_p], axis=1).astype(np.float32)
        news_ctx['emb'] = list(emb_np)

        D = 2*text_dim + 2*n_topics
        ctx_cols = [f'ctx_{i}' for i in range(D)]
        ctx_rows, diag_counts = [], []

        for row in candles.itertuples(index=False):
            tkr = row.ticker_norm
            cutoff = row.date - pd.Timedelta(days=1)
            mask = (news_ctx['publish_date'] <= cutoff) & (news_ctx['tickers_list'].apply(lambda lst: tkr in lst))
            past = news_ctx[mask]
            diag_counts.append(int(len(past)))
            if len(past) > max_news_per_sample:
                past = past.tail(max_news_per_sample)

            if len(past):
                # –ø–µ—Ä–µ–Ω–æ—Å–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ xp –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (–µ—Å–ª–∏ GPU)
                E = np.stack(past['emb'].values, axis=0).astype(np.float32)
                if dev == "gpu":
                    import cupy as cp
                    E_xp = cp.asarray(E)
                    T = pd.to_datetime(past['publish_date']).values.astype('datetime64[ns]')
                    ctx_xp = transformer_like_pool(E_xp, T, cutoff, tau_days=tau_days, time_bias=1.0, xp=cp)
                    ctx = cp.asnumpy(ctx_xp)
                else:
                    T = pd.to_datetime(past['publish_date']).values.astype('datetime64[ns]')
                    ctx = transformer_like_pool(E, T, cutoff, tau_days=tau_days, time_bias=1.0)
            else:
                ctx = np.zeros((D,), dtype=np.float32)
            ctx_rows.append(ctx)

        ctx_df = pd.DataFrame(np.vstack(ctx_rows), columns=ctx_cols, index=candles.index)
        out_parts += [ctx_df, pd.DataFrame({'ctx_news_count': diag_counts}, index=candles.index)]

    # =========================
    # B) –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (—Ç–µ–º—ã —Å—á–∏—Ç–∞–µ–º –ø–æ –û–ë–û–ò–ú —Ç–µ–∫—Å—Ç–∞–º)
    # =========================
    if add_aditional_emb:
        vec_feat_t = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['title'].astype(str).fillna('')
        )
        _, xt_t = vec_feat_t.transform(news['title'].astype(str).fillna(''))

        vec_feat_p = NewsVectorizer(text_dim=text_dim, n_topics=n_topics, device=device).fit(
            news['publication'].astype(str).fillna('')
        )
        _, xt_p = vec_feat_p.transform(news['publication'].astype(str).fillna(''))

        topic_vec = (xt_t + xt_p) / 2.0

        news_feat = news[['publish_date','title','publication','tickers_list']].copy()
        news_feat['topic_vec'] = list(topic_vec.astype(np.float32))

        extra_cols = (
            ['news_count_day','dup_ratio',
             'title_len_mean','title_len_std',
             'publication_len_mean','publication_len_std',
             'top_topic_idx','top_topic_strength','news_spike_rough'] +
            [f'topic_mean_{i}' for i in range(n_topics)]
        )
        extra_rows = []

        for row in candles.itertuples(index=False):
            tkr = row.ticker_norm
            cutoff = row.date - pd.Timedelta(days=1)

            mask_has = news_feat['tickers_list'].apply(lambda lst: tkr in lst)
            day_news = news_feat[(news_feat['publish_date'].dt.normalize() == cutoff) & mask_has].copy()
            last60   = news_feat[(news_feat['publish_date'] > (cutoff - pd.Timedelta(days=60))) &
                                 (news_feat['publish_date'] <= cutoff) & mask_has].copy()

            extra = make_extra_news_features(day_news, last60, n_topics)
            extra_rows.append([extra.get(c, 0.0) for c in extra_cols])

        extra_df = pd.DataFrame(extra_rows, columns=extra_cols, index=candles.index)
        out_parts.append(extra_df)

    enriched = pd.concat(out_parts, axis=1).drop(columns=['ticker_norm'], errors='ignore')

    if out_csv:
        Path(out_csv).parent.mkdir(parents=True, exist_ok=True)
        enriched.to_csv(out_csv, index=False)

    return enriched

enriched = add_news_context_simple(
    candles_csv="candles.csv",
    news_csv="fast_train_news_with_sent.csv",
    out_csv="candles_features.csv",
    text_dim=64,
    n_topics=12,
    max_news_per_sample=128,
    tau_days=7.0,
    add_context_emb = False,
    device="auto",
)

enriched = add_news_context_simple(
    candles_csv="candles_2.csv",
    news_csv="fast_test_news_with_sent.csv",
    out_csv="candles_2_features.csv",
    text_dim=64,
    n_topics=12,
    max_news_per_sample=128,
    tau_days=7.0,
    add_context_emb = False,
    device="auto",
)

"""### 4 —á–∞—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è - –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è submissions.csv"""

# -----------------------------
# 0) –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ
# -----------------------------
def _safe_parse_tickers(val):
    """–ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –ø–æ–ª–µ 'tickers' –≤ —Å–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤."""
    if pd.isna(val):
        return []
    s = str(val).strip()
    # 1) –ø–æ–ø—ã—Ç–∫–∞ –∫–∞–∫ Python-—Å–ø–∏—Å–æ–∫
    try:
        obj = ast.literal_eval(s)
        if isinstance(obj, (list, tuple)):
            return [str(x).strip().upper() for x in obj if str(x).strip()]
    except Exception:
        pass
    # 2) –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞: —É–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏/–∫–∞–≤—ã—á–∫–∏ –∏ —Ä–µ–∂–µ–º –ø–æ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã–º
    s = re.sub(r"[\[\]'\"{}]", " ", s)
    tokens = re.split(r"[^A-Za-z0-9\.]+", s)
    return [t.upper() for t in tokens if t.strip()]

# -----------------------------
# 1) –†–∞–∑–º–µ—Ç–∫–∞ —Ç–∞—Ä–≥–µ—Ç–æ–≤ R_1..R_20
# -----------------------------
def make_targets(df: pd.DataFrame, max_horizon: int = 20) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ R_1..R_max_horizon:
      R_i(t) = close_{t+i} / close_t - 1  (–Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∑–∞ i –¥–Ω–µ–π)
    –î–µ–ª–∞–µ—Ç –≤—Å—ë –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–∫–µ—Ä—É –æ—Ç–¥–µ–ª—å–Ω–æ.
    """
    out = df.copy()
    out = out.sort_values(["ticker", "begin"]).reset_index(drop=True)

    g = out.groupby("ticker", group_keys=False)
    for i in range(1, max_horizon + 1):
        out[f"R_{i}"] = g["close"].shift(-i) / out["close"] - 1.0

    return out

# -----------------------------------
# 2) –§–∏—á–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ –∏ rolling)
# -----------------------------------
def build_news_features(
    news_csv: str,
    roll_days=(3, 7),
    use_cols=("p_positive", "p_negative", "p_neutral", "sent_score"),
):
    """
    –ì–æ—Ç–æ–≤–∏—Ç —Ñ–∏—á–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π:
    - —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ç–∏–∫–µ—Ä (explode)
    - –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ (ticker, date): —Å—Ä–µ–¥–Ω–µ–µ/–º–∞–∫—Å/–º–∏–Ω/sum sent_score, —Å—Ä–µ–¥–Ω–∏–µ p_*, n_news
    - rolling –ø–æ –ø—Ä–æ—à–ª—ã–º N –¥–Ω—è–º, –°–î–í–ò–ù–£–¢–´–ô –Ω–∞ 1 –¥–µ–Ω—å (anti-leak)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
      ['ticker','date', base_feats..., <base>_lag1, <base>_roll{R}, ...]
    """
    print("üì• Loading news...")
    news = pd.read_csv(news_csv)
    news = news.drop(columns=["Unnamed: 0"], errors="ignore")

    news["publish_date"] = pd.to_datetime(news["publish_date"], errors="coerce")
    news = news.dropna(subset=["publish_date"])
    news["date"] = news["publish_date"].dt.normalize()

    # —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ç–∏–∫–µ—Ä—ã –∏ explode
    tqdm.pandas(desc="üß∑ Parse tickers")
    news["ticker_list"] = news["tickers"].progress_apply(_safe_parse_tickers)
    news = news.explode("ticker_list").rename(columns={"ticker_list": "ticker"})
    news["ticker"] = news["ticker"].astype(str)
    news = news[news["ticker"].str.len() > 0]

    # –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    keep = ["ticker", "date", *use_cols]
    news = news[keep].copy()

    # –±–∞–∑–æ–≤—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ –¥–Ω—é
    agg_map = {
        "sent_score": ["mean", "max", "min", "sum"],
        "p_positive": ["mean"],
        "p_negative": ["mean"],
        "p_neutral": ["mean"],
    }
    print("üìä Daily aggregations...")
    daily = (
        news.groupby(["ticker", "date"])
        .agg(agg_map)
        .rename(columns={"mean": "mean", "max": "max", "min": "min", "sum": "sum"})
    )
    daily.columns = [f"{c0}_{c1}" for (c0, c1) in daily.columns.to_flat_index()]
    daily = daily.reset_index()

    # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
    cnt = news.groupby(["ticker", "date"]).size().reset_index(name="n_news")
    daily = daily.merge(cnt, on=["ticker", "date"], how="left")

    # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö —Ñ–∏—á–µ–π
    for c in ["sent_score_mean", "sent_score_max", "sent_score_min", "sent_score_sum",
              "p_positive_mean", "p_negative_mean", "p_neutral_mean", "n_news"]:
        if c not in daily.columns:
            daily[c] = 0.0

    # rolling –ø–æ —Ç–∏–∫–µ—Ä–∞–º, —Å–¥–≤–∏–≥ –Ω–∞ 1 –¥–µ–Ω—å (—Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–µ)
    if roll_days:
        print("üîÅ Rolling features...")
        pieces = []
        g = daily.sort_values(["ticker", "date"]).groupby("ticker", group_keys=False)
        for ticker, df_t in tqdm(g, total=g.ngroups, desc="Rolling per ticker"):
            df_t = df_t.sort_values("date").copy()
            base_cols = [
                "sent_score_mean", "sent_score_max", "sent_score_min", "sent_score_sum",
                "p_positive_mean", "p_negative_mean", "p_neutral_mean", "n_news",
            ]
            # –ª–∞–≥ –Ω–∞ 1 –¥–µ–Ω—å
            for col in base_cols:
                df_t[f"{col}_lag1"] = df_t[col].shift(1)
            # —Å—Ä–µ–¥–Ω–∏–µ –ø–æ –æ–∫–Ω–∞–º, —Ç–æ–∂–µ —Å–¥–≤–∏–Ω—É—Ç—ã–µ
            for R in roll_days:
                for col in base_cols:
                    df_t[f"{col}_roll{R}"] = (
                        df_t[col].rolling(window=R, min_periods=1).mean().shift(1)
                    )
            pieces.append(df_t)
        daily = pd.concat(pieces, ignore_index=True)

    return daily  # ['ticker','date', base_today..., *_lag1, *_rollR]

# -----------------------------------
# 2.1) –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ —Ñ–∏—á–∏
# -----------------------------------
def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–æ–ª–±—Ü–∞ 'date' (–¥–∞—Ç–∞ –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏).
    """
    out = df.copy()
    out["ds"] = pd.to_datetime(out["date"])
    out["dow"] = out["ds"].dt.weekday               # 0..6
    out["month"] = out["ds"].dt.month               # 1..12
    out["is_month_start"] = out["ds"].dt.is_month_start.astype(int)
    out["is_month_end"]   = out["ds"].dt.is_month_end.astype(int)
    # —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
    out["dow_sin"] = np.sin(2*np.pi*out["dow"]/7.0)
    out["dow_cos"] = np.cos(2*np.pi*out["dow"]/7.0)
    return out

# -----------------------------------
# 3) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã X / Y / —Å–ø–ª–∏—Ç
# -----------------------------------
def build_xy(df_feat: pd.DataFrame, max_horizon: int = 20):
    """
    –°–æ–±–∏—Ä–∞–µ–º X (—Ñ–∏—á–∏) –∏ Y (–º—É–ª—å—Ç–∏–≤—ã—Ö–æ–¥–Ω–æ–π —Ç–∞—Ä–≥–µ—Ç R_1..R_max_horizon).
    –§–∏—á–∏: –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫—Ä–æ–º–µ R_*, begin (–≤—Ä–µ–º–µ–Ω–∏) –∏, –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤.
    """
    targets = [f"R_{i}" for i in range(1, max_horizon + 1)]
    # –æ—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –≤—Å–µ —Ç–∞—Ä–≥–µ—Ç—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç (—á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ –ø–æ–ª–Ω—ã–π –≤–µ–∫—Ç–æ—Ä)
    df = df_feat.dropna(subset=targets).copy()

    # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    X = df.select_dtypes(include=[np.number]).copy()
    # –∏—Å–∫–ª—é—á–∏–º —Å–∞–º–∏ —Ç–∞—Ä–≥–µ—Ç—ã –∏–∑ X
    X = X.drop(columns=[c for c in X.columns if c.startswith("R_")], errors="ignore")
    # begin/–∏–Ω–¥–µ–∫—Å—ã/—Å–ª—É–∂–µ–±–Ω—ã–µ ‚Äî —Ç–æ–∂–µ –Ω–µ –Ω—É–∂–Ω—ã –≤ X
    X = X.drop(columns=["Unnamed: 0"], errors="ignore")

    Y = df[targets].astype(float).copy()

    # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤: –º–µ–¥–∏–∞–Ω—ã –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º train (–ø–æ—Å—á–∏—Ç–∞–µ–º –ø–æ–∑–∂–µ)
    print(f'Finel X size {X.shape}')
    return df, X, Y

def time_train_test_split(df: pd.DataFrame, test_frac: float = 0.2):
    """
    –î–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏: –ø–æ—Å–ª–µ–¥–Ω–∏–µ test_frac –¥–∞—Ç ‚Äî –≤ —Ç–µ—Å—Ç.
    –†–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∏–∫–µ—Ä–æ–≤ (—Å–ø–ª–∏—Ç –ø–æ –æ–±—â–µ–π –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏).
    """
    df = df.sort_values("begin").reset_index(drop=True)
    uniq_dates = np.sort(df["begin"].dt.normalize().unique())
    split_idx = int(len(uniq_dates) * (1.0 - test_frac))
    split_date = uniq_dates[split_idx] if split_idx < len(uniq_dates) else uniq_dates[-1]
    train_mask = df["begin"].dt.normalize() < split_date
    test_mask = ~train_mask
    return train_mask, test_mask, pd.Timestamp(split_date)

def _parse_tickers_cell(x):
    if isinstance(x, list):
        return [str(t).strip().upper() for t in x]
    s = str(x)
    try:
        import ast
        v = ast.literal_eval(s)
        if isinstance(v, (list, tuple)):
            return [str(t).strip().upper() for t in v]
    except Exception:
        pass
    return [tok.strip().upper() for tok in s.replace("[","").replace("]","").replace("'","").split(",") if tok.strip()]

def _one_hot_label(lbl: str):
    lbl = str(lbl).lower()
    return (
        1.0 if lbl == "positive" else 0.0,
        1.0 if lbl == "negative" else 0.0,
        1.0 if lbl == "neutral"  else 0.0,
    )

def _build_news_tails(news: pd.DataFrame, N_NEWS: int = 10) -> tuple[dict, np.ndarray]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      tails_by_ticker: { 'GAZP': np.array(N_NEWS,7), ... }
      global_tail:     np.array(N_NEWS,7)
    –ì–¥–µ –∫–∞–∂–¥–∞—è –Ω–æ–≤–æ—Å—Ç—å -> –≤–µ–∫—Ç–æ—Ä [p_pos, p_neg, p_neu, sent_score, 1{pos},1{neg},1{neu}]
    """
    if news is None or len(news) == 0:
        # –ø—É—Å—Ç–æ ‚Äî –≤–µ—Ä–Ω—ë–º –Ω—É–ª–∏
        return {}, np.zeros((N_NEWS, 7), dtype=np.float32)

    df = news.copy()
    if "publish_date" in df.columns:
        df["publish_date"] = pd.to_datetime(df["publish_date"], errors="coerce")
    else:
        df["publish_date"] = pd.Timestamp(0)

    df["tickers_list"] = df["tickers"].apply(_parse_tickers_cell)
    df = df.explode("tickers_list", ignore_index=True)
    df.rename(columns={"tickers_list":"ticker"}, inplace=True)
    df["ticker"] = df["ticker"].astype(str).str.upper()
    df = df.sort_values("publish_date")

    # —á–∏—Å–ª–æ–≤—ã–µ
    for c in ["p_positive","p_negative","p_neutral","sent_score"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
        else:
            df[c] = 0.0
    if "sent_label" not in df.columns:
        df["sent_label"] = "neutral"

    def news_rows_to_seq(dfi: pd.DataFrame, n: int) -> np.ndarray:
        tail = dfi.tail(n)
        rows = []
        for _, r in tail.iterrows():
            v = [
                float(r.get("p_positive", 0.0)),
                float(r.get("p_negative", 0.0)),
                float(r.get("p_neutral",  0.0)),
                float(r.get("sent_score", 0.0)),
            ]
            v += list(_one_hot_label(r.get("sent_label", "")))
            rows.append(v)
        seq = np.array(rows, dtype=np.float32)
        # –ø–∞–¥–¥–∏–Ω–≥ —Å–≤–µ—Ä—Ö—É –Ω—É–ª—è–º–∏ –¥–æ (n,7)
        if len(seq) < n:
            pad = np.zeros((n - len(seq), 7), dtype=np.float32)
            seq = np.vstack([pad, seq])
        return seq

    global_tail = news_rows_to_seq(df, N_NEWS)

    tails_by_ticker = {}
    for tkr, dfi in df.groupby("ticker"):
        tails_by_ticker[tkr] = news_rows_to_seq(dfi, N_NEWS)

    return tails_by_ticker, global_tail

def _ensure_numeric(X_train: pd.DataFrame, X_test: pd.DataFrame):
    """
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Ñ–∏—á –∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç NaN.
    """
    # 1) —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ
    Xtr = X_train.select_dtypes(include=[np.number]).copy()
    Xte = X_test.select_dtypes(include=[np.number]).copy()

    # 2) –≤—ã—Ä–æ–≤–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —Ç–µ—Å—Ç–∞ –ø–æ–¥ —Ç—Ä–µ–π–Ω
    Xte = Xte.reindex(columns=Xtr.columns, fill_value=0.0)

    # 3) –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–∞–º–∏ train
    fillna_vals = Xtr.median(numeric_only=True)
    Xtr = Xtr.fillna(fillna_vals)
    Xte = Xte.fillna(fillna_vals)

    return Xtr, Xte

def get_pred(X_train, y_train, X_test, news=None, model_kind="gbr", random_state=42):
    """
    –ú—É–ª—å—Ç–∏–≤—ã—Ö–æ–¥–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–æ–º:
      - –ß–∏—Å–ª–µ–Ω–Ω—ã–µ: median-impute
      - –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ (object/category): OneHot (dense), handle_unknown='ignore'
      - –î–∞—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: –¥—Ä–æ–ø–∞–µ–º
    """
    # --- –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
    if model_kind == "gbr":
        base = GradientBoostingRegressor(random_state=random_state)
        need_dense_ohe = True
    elif model_kind == "rf":
        base = RandomForestRegressor(
            n_estimators=500, max_depth=None, n_jobs=-1, random_state=random_state
        )
        need_dense_ohe = True
    elif model_kind == "ridge":
        base = Ridge(alpha=1.0, random_state=random_state)
        need_dense_ohe = True  # –º–æ–∂–Ω–æ –∏ sparse, –Ω–æ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Å—Ç–∞–≤–∏–º dense
    else:
        raise ValueError(f"Unknown model_kind: {model_kind}")

    est = MultiOutputRegressor(base)

    # --- –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ DataFrame/ndarray
    if not isinstance(X_train, pd.DataFrame):
        X_train = pd.DataFrame(X_train)
    if not isinstance(X_test, pd.DataFrame):
        X_test = pd.DataFrame(X_test)

    # --- —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    dt_cols = X_train.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
    # –≤—ã–±—Ä–æ—Å–∏–º –¥–∞—Ç—ã (–µ—Å–ª–∏ –∏—Ö –Ω–∞–¥–æ ‚Äî –∑–∞—Ä–∞–Ω–µ–µ –ø—Ä–µ–≤—Ä–∞—Ç–∏ –≤ —á–∏—Å–ª–∞)
    if dt_cols:
        X_train = X_train.drop(columns=dt_cols, errors="ignore")
        X_test  = X_test.drop(columns=dt_cols, errors="ignore")

    num_cols = X_train.select_dtypes(include=["number", "bool"]).columns.tolist()
    cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()

    # –Ω–∞ —Ç–µ—Å—Ç–µ –∏–Ω–æ–≥–¥–∞ –ø–æ—è–≤–ª—è—é—Ç—Å—è –Ω–æ–≤—ã–µ —á–∏—Å—Ç–æ NaN –∫–æ–ª–æ–Ω–∫–∏/—Ç–∏–ø—ã ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º
    for c in X_test.columns:
        if c not in num_cols and c not in cat_cols:
            if pd.api.types.is_numeric_dtype(X_test[c]) or pd.api.types.is_bool_dtype(X_test[c]):
                num_cols.append(c)
            elif pd.api.types.is_object_dtype(X_test[c]) or pd.api.types.is_categorical_dtype(X_test[c]):
                cat_cols.append(c)

    # ‚Äî –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=not need_dense_ohe and False)
    # –≤—ã—à–µ –æ—Å—Ç–∞–≤–∏–ª–∏ dense (sparse_output=False), —Ç.–∫. GBR –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç sparse

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", SimpleImputer(strategy="median"), num_cols),
            ("cat", Pipeline([("imputer", SimpleImputer(strategy="most_frequent")), ("ohe", ohe)]), cat_cols),
        ],
        remainder="drop",
        verbose_feature_names_out=False,
    )

    pipe = Pipeline(steps=[("prep", preprocessor), ("model", est)])

    # --- y –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (n_samples, n_targets)
    y_arr = np.asarray(y_train)
    if y_arr.ndim == 1:
        y_arr = y_arr.reshape(-1, 1)

    pipe.fit(X_train, y_arr)

    pred_train = pipe.predict(X_train)
    pred_test  = pipe.predict(X_test)
    return pred_train, pred_test

def get_pred_simple(X_train, y_train, X_test, model_kind="gbr", random_state=42):
    """
    –ú—É–ª—å—Ç–∏–≤—ã—Ö–æ–¥–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:
      - "gbr": GradientBoostingRegressor
      - "rf":  RandomForestRegressor
      - "ridge": Ridge
    """
    if model_kind == "gbr":
        base = GradientBoostingRegressor(random_state=random_state)
    elif model_kind == "rf":
        base = RandomForestRegressor(
            n_estimators=500, max_depth=None, n_jobs=-1, random_state=random_state
        )
    elif model_kind == "ridge":
        base = Ridge(alpha=1.0, random_state=random_state)
    else:
        raise ValueError(f"Unknown model_kind: {model_kind}")

    model = MultiOutputRegressor(base)
    model.fit(X_train, y_train)
    return model.predict(X_train), model.predict(X_test)

from sklearn.metrics import mean_absolute_error
import numpy as np
import pandas as pd

def evaluate(y_true: np.ndarray,
             y_pred: np.ndarray,
             tag: str = "TEST",
             p_pred: np.ndarray | None = None,
             prob_sensitivity: float = 12.0,
             print_table: bool = True):
    """
    –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –≥–æ—Ä–∏–∑–æ–Ω—Ç—É i –∏ –∏—Ç–æ–≥–æ–≤—ã–π Score_i:
      Score_i = 0.7 * MAE_norm_i + 0.3 * Brier_norm_i + 0.1 * (1 / DA_i)

    –ì–¥–µ:
      MAE_norm_i   = 1 - (MAE_i / MAE_base_i),       MAE_base_i = mean(|y_i|)
      Brier_norm_i = 1 - (Brier_i / Brier_base_i),   Brier_base_i = 0.25  (p_base=0.5)
      DA_i         = mean(sign(y_i) == sign(yhat_i))  (accuracy –ø–æ –∑–Ω–∞–∫—É)

    –ï—Å–ª–∏ p_pred –Ω–µ –∑–∞–¥–∞–Ω–æ, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞ –±–µ—Ä—ë–º –∫–∞–∫:
      p_pred = sigmoid(prob_sensitivity * y_pred)  (–ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ)
    """
    eps = 1e-9
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    assert y_true.shape == y_pred.shape and y_true.ndim == 2, "y_true/y_pred –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å (N, H)"

    N, H = y_true.shape

    # --- –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞ ---
    if p_pred is None:
        p_pred = 1.0 / (1.0 + np.exp(-prob_sensitivity * y_pred))
    else:
        p_pred = np.asarray(p_pred)
        if p_pred.shape != (N, H):
            raise ValueError("p_pred –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ñ–æ—Ä–º—É (N, H)")

    y_up = (y_true > 0).astype(float)

    maes, maes_base, mae_norms = [], [], []
    briers, briers_base, brier_norms = [], [], []
    das, scores = [], []

    for i in range(H):
        yt = y_true[:, i]
        yp = y_pred[:, i]
        pp = p_pred[:, i]
        yi = y_up[:, i]

        # --- MAE –∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ ---
        mae_i = mean_absolute_error(yt, yp)
        mae_base_i = np.mean(np.abs(yt))
        mae_norm_i = 1.0 - mae_i / (mae_base_i + eps)

        # --- Brier –∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ ---
        brier_i = np.mean((yi - pp) ** 2)
        brier_base_i = 0.25  # –ø—Ä–∏ p_base = 0.5
        brier_norm_i = 1.0 - brier_i / (brier_base_i + eps)

        # --- Accuracy –ø–æ –∑–Ω–∞–∫—É (DA) ---
        da_i = np.mean(np.sign(yt) == np.sign(yp))

        # --- –ò—Ç–æ–≥–æ–≤—ã–π Score ---
        score_i = 0.7 * mae_norm_i + 0.3 * brier_norm_i + 0.1 * (1.0 / (da_i + eps))

        maes.append(mae_i); maes_base.append(mae_base_i); mae_norms.append(mae_norm_i)
        briers.append(brier_i); briers_base.append(brier_base_i); brier_norms.append(brier_norm_i)
        das.append(da_i); scores.append(score_i)

    maes = np.array(maes); maes_base = np.array(maes_base); mae_norms = np.array(mae_norms)
    briers = np.array(briers); briers_base = np.array(briers_base); brier_norms = np.array(brier_norms)
    das = np.array(das); scores = np.array(scores)

    if print_table:
        print(f"\n=== METRICS ({tag}) ===")
        header = "Hor |     MAE | MAE_base | MAE_norm |   Brier | Brier_base | Brier_norm |   DA   |    Score"
        print(header)
        for i in range(H):
            print(f"R_{i+1:>2} | {maes[i]:8.6f} | {maes_base[i]:8.6f} | {mae_norms[i]:8.4f} | "
                  f"{briers[i]:8.6f} | {briers_base[i]:11.6f} | {brier_norms[i]:10.4f} | "
                  f"{das[i]:6.3f} | {scores[i]:9.4f}")
        print("-" * len(header))
        print(f"AVG | {maes.mean():8.6f} | {maes_base.mean():8.6f} | {mae_norms.mean():8.4f} | "
              f"{briers.mean():8.6f} | {briers_base.mean():11.6f} | {brier_norms.mean():10.4f} | "
              f"{das.mean():6.3f} | {scores.mean():9.4f}")
        print(f"SUM Score over horizons: {scores.sum():.4f}")

    # –í–µ—Ä–Ω—ë–º —Å—Ä–µ–¥–Ω–∏–π –∏—Ç–æ–≥–æ–≤—ã–π score –∏ DataFrame —Å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    details = pd.DataFrame({
        "horizon": [f"R_{i}" for i in range(1, H+1)],
        "MAE": maes, "MAE_base": maes_base, "MAE_norm": mae_norms,
        "Brier": briers, "Brier_base": briers_base, "Brier_norm": brier_norms,
        "DA": das, "Score": scores
    })
    return scores.mean(), details

def generate_submissions(
    candles_path: str = "candles.csv",
    candles2_path: str = "candles_2.csv",
    news_path: str | None = "news.csv",
    news2_path: str | None = "news_2.csv",
    max_horizon: int = 20,
    submission_csv: str = "submissions.csv",
):
    """
    1) –û–±—É—á–∞–µ–º—Å—è –Ω–∞ candles.csv (+news.csv, –µ—Å–ª–∏ –µ—Å—Ç—å)
    2) –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è –ü–û–°–õ–ï–î–ù–ï–ô —Å–≤–µ—á–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –∏–∑ candles_2.csv (+news_2.csv, –µ—Å–ª–∏ –µ—Å—Ç—å)
    3) –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Ñ–æ—Ä–º–∞—Ç–∞: ticker,p1..p20
    """
    # ---------- load ----------
    df_tr = pd.read_csv(candles_path)
    df_te = pd.read_csv(candles2_path)

    for d in (df_tr, df_te):
        d["begin"] = pd.to_datetime(d["begin"], errors="coerce")
        d["ticker"] = d["ticker"].astype(str).str.upper()

    # ---------- train targets & X/Y ----------
    df_tr_lbl = make_targets(df_tr, max_horizon=max_horizon)
    df_train, X_train, Y_train = build_xy(df_tr_lbl, max_horizon=max_horizon)

    # –¥–æ–±–∞–≤–∏–º —Ç–∏–∫–µ—Ä –≤ X (—á—Ç–æ–±—ã get_pred –º–æ–≥ –ø—Ä–∏–∫–ª–µ–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–∏–∫–µ—Ä—É)
    X_train_w = X_train.copy()
    if "ticker" not in X_train_w.columns and "ticker" in df_train.columns:
        X_train_w["ticker"] = df_train["ticker"].values

    # simple imputing
    fillna_vals = X_train_w.select_dtypes(include=[np.number]).median(numeric_only=True)
    X_train_w[X_train_w.select_dtypes(include=[np.number]).columns] = (
        X_train_w.select_dtypes(include=[np.number]).fillna(fillna_vals)
    )

    # ---------- test rows: –æ–¥–Ω–∞ (–ø–æ—Å–ª–µ–¥–Ω—è—è) —Å–≤–µ—á–∞ –Ω–∞ —Ç–∏–∫–µ—Ä ----------
    te_sorted = df_te.sort_values(["ticker", "begin"])
    te_last = te_sorted.groupby("ticker").tail(1).copy()

    X_test = te_last.select_dtypes(include=[np.number]).copy()
    X_test = X_test.drop(columns=["Unnamed: 0"], errors="ignore")
    X_test["ticker"] = te_last["ticker"].values

    # –ø—Ä–∏–≤–µ—Å—Ç–∏ –Ω–∞–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ X_test –∫ train
    for c in X_train_w.columns:
        if c not in X_test.columns and c != "ticker":
            X_test[c] = 0.0
    extra_in_test = [c for c in X_test.columns if c not in X_train_w.columns]
    if extra_in_test:
        X_test = X_test.drop(columns=extra_in_test, errors="ignore")
    X_test = X_test.reindex(columns=X_train_w.columns)

    num_cols_te = X_test.select_dtypes(include=[np.number]).columns
    X_test[num_cols_te] = X_test[num_cols_te].fillna(fillna_vals)

    # ---------- –Ω–æ–≤–æ—Å—Ç–∏ ----------
    if news_path is not None:
        try:
            news_tr = pd.read_csv(news_path)
        except Exception:
            news_tr = pd.DataFrame({"tickers": [], "publish_date": []})
    else:
        news_tr = pd.DataFrame({"tickers": [], "publish_date": []})

    if news2_path is not None:
        try:
            news_te = pd.read_csv(news2_path)
        except Exception:
            news_te = pd.DataFrame({"tickers": [], "publish_date": []})
    else:
        news_te = pd.DataFrame({"tickers": [], "publish_date": []})

    news_all = pd.concat([news_tr, news_te], ignore_index=True)

    # ---------- –º–æ–¥–µ–ª—å –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ----------
    use_simple = (len(news_all) == 0) and callable(globals().get("get_pred_simple"))

    if use_simple:
        pred_train, pred_test = get_pred_simple(X_train_w, Y_train, X_test)
    else:
        pred_train, pred_test = get_pred(X_train_w, Y_train, X_test, news_all)

    # ---------- EVALUATE (–Ω–∞ –æ–±—É—á–µ–Ω–∏–∏) ----------
    # –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    evaluate(Y_train.values, pred_train, tag="TRAIN")

    # ---------- —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å submissions ----------
    sub = pd.DataFrame({"ticker": te_last["ticker"].values})
    for i in range(1, max_horizon + 1):
        sub[f"p{i}"] = pred_test[:, i - 1]
    sub = sub.sort_values("ticker").reset_index(drop=True)

    Path(submission_csv).parent.mkdir(parents=True, exist_ok=True)
    sub.to_csv(submission_csv, index=False)
    print(f"‚úÖ Saved {submission_csv} (rows: {len(sub)})")
    return sub

_ = generate_submissions(
    candles_path="candles_features.csv",
    candles2_path="candles_2_features.csv",
    news_path="fast_train_news_with_sent.csv",        # –º–æ–∂–Ω–æ None
    news2_path="fast_test_news_with_sent.csv",     # –º–æ–∂–Ω–æ None
    max_horizon=20,
    submission_csv="submissions.csv",
)